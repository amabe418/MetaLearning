\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\addto\captionsspanish{\renewcommand{\abstractname}{Abstract}}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{float}  
\usepackage{lipsum}
\usepackage[authoryear]{natbib}
\usepackage[colorlinks=true,
            linkcolor=blue,
            citecolor=blue,
            urlcolor=blue]{hyperref}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{array}


\geometry{a4paper, left=2.5cm, right=2.5cm, top=2.5cm, bottom=2.5cm}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=blue,
    citecolor=blue
}

\lstset{style=mystyle}


\begin{document}

\begin{titlepage}
    \centering

    \vspace*{-1cm}
    \includegraphics[width=4cm]{matcom.jpeg}
    \vspace{1cm}

    {\Large \textbf{Universidad de La Habana}}\\[0.3cm]
    {\large \textbf{Facultad de Matemática y Computación}}\\[1cm]

    {\large Asignatura: Aprendizaje Autom\'atico}\\[2cm]

    {\LARGE \textbf{Meta-Learning}}\\[0.8cm]

    {\Large \textbf{Integrantes}}\\[0.6cm]
    {\large Jabel Resendiz Aguirre}\\
    {\large Amalia Beatriz Valiente Hinojosa}\\
    {\large Noel Pérez Calvo}\\
    {\large Melanie Forsythe Matos}\\
    {\large Jorge Alejandro Echevarr\'ia Brunet}\\
    {\large Arianne Camila Palancar Ochando}\\[1.7cm]

    {\large Carrera: Ciencia de la Computación}\\[2cm]

    \vfill
    {\large \today}
\end{titlepage}

\begin{abstract}
Este trabajo aborda el problema de AutoML, proponiendo un enfoque basado en meta-learning que aprende 
meta-features de datasets (MetaFeatX) y utiliza transfer-learning para predecir configuraciones óptimas 
de hiperparámetros. Se presenta la motivación del proyecto, los problemas que resuelve, y se muestran 
resultados experimentales en comparación con sistemas AutoML tradicionales. La combinación de 
MetaFeatX y transfer-learning permite un AutoML más eficiente, interpretable y con menor costo computacional.
\end{abstract}

\tableofcontents
\newpage


\section{Introducción}
\label{sec:intro}
El \textbf{meta-learning}, o ``aprendizaje a aprender'', es un área del aprendizaje automático que busca 
desarrollar algoritmos capaces de \textbf{aprender de experiencias pasadas para mejorar el rendimiento 
en nuevas tareas}. A diferencia del aprendizaje tradicional, donde un modelo se entrena para una 
tarea específica, el meta-learning se centra en \textbf{extraer conocimiento generalizable} que pueda 
transferirse a problemas desconocidos, acelerando el proceso de aprendizaje y mejorando la eficiencia.

En el contexto de \textbf{AutoML} (Automated Machine Learning), el meta-learning permite \textbf{seleccionar automáticamente algoritmos y configuraciones de hiperparámetros} adecuadas para un conjunto de datos dado, basándose en información obtenida de datasets anteriores. Esto reduce significativamente el tiempo de experimentación y evita la necesidad de un ajuste manual exhaustivo de los modelos.



\paragraph{Conceptos Clave: }Para comprender meta-learning y el enfoque que se presenta en este reporte, es importante familiarizarse
 con los siguientes conceptos:

\begin{itemize}
    \item \textbf{Meta-features:} Son características que describen datasets. Por ejemplo, el número de 
    instancias, el número de atributos, la distribución de las clases o medidas estadísticas de los 
    atributos. Las meta-features permiten comparar datasets y predecir qué algoritmos o configuraciones 
    funcionarán mejor.
    
    \item \textbf{Pipeline de Machine Learning:} Es la secuencia de pasos que se aplican a un dataset, 
    desde la preparación de datos (preprocesamiento, normalización) hasta el entrenamiento y evaluación
    de un modelo. Cada etapa puede tener múltiples opciones e \textbf{hiperparámetros} que afectan el 
    rendimiento final.

    \item \textbf{Hiperparámetros:} Son parámetros de un algoritmo de ML que no se aprenden directamente 
    a partir de los datos, sino que deben fijarse antes del entrenamiento. Ejemplos incluyen la 
    profundidad máxima de un árbol de decisión, la tasa de aprendizaje de un modelo de redes neuronales 
    o el número de vecinos en un k-NN. La correcta elección de hiperparámetros es crucial para el 
    desempeño de un modelo.

    \item \textbf{Tareas de aprendizaje:} Se refiere a un problema específico de aprendizaje automático
    que se desea resolver, definido por un dataset y un objetivo de predicción (por ejemplo, 
    clasificación o regresión). En meta-learning, cada tarea puede considerarse como una instancia en la
    que el sistema debe seleccionar un algoritmo y sus hiperparámetros adecuados. El aprendizaje meta 
    busca \textbf{transferir conocimiento entre tareas} para mejorar la eficiencia en tareas nuevas.

    \item \textbf{AutoML:} Se refiere a la automatización del proceso de selección de algoritmos y 
    ajuste de hiperparámetros. Los sistemas AutoML tradicionales requieren probar múltiples 
    configuraciones y evaluar su desempeño, lo que puede ser costoso en tiempo y recursos.
\end{itemize}

\paragraph{Motivación y Problema}  
% \subsection{Motivación y Problema}
El principal desafío que aborda este proyecto es: \textit{cómo lograr que un sistema AutoML prediga de manera eficiente qué algoritmos y configuraciones funcionarán bien en un nuevo dataset}, sin tener que evaluar exhaustivamente todas las posibilidades. Los problemas principales incluyen:

\begin{itemize}
    \item \textbf{Cold-start:} Para un dataset nuevo, no se conoce previamente qué configuraciones funcionarán, lo que obliga a probar muchas combinaciones costosas.  
    \item \textbf{Diseño de meta-features:} No siempre las meta-features manuales garantizan buena generalización entre datasets.  
    \item \textbf{Espacio de datasets complejo:} La diversidad de datasets y algoritmos hace difícil estimar qué configuraciones funcionarán bien.
\end{itemize}

Aunque existen sistemas como AutoSkLearn, PMF u OBOE que implementan estrategias de selección y 
optimización, estos enfoques requieren lanzar experimentos para cada nuevo dataset (fase cold-start), 
lo que limita la eficiencia. Además, el diseño manual de meta-features no siempre garantiza una buena 
generalización entre datasets.

Por esta razón, surge la necesidad de \textbf{aprender meta-features que capturen la relación entre datasets y configuraciones óptimas de hiperparámetros}. 
Estas meta-features permiten:

\begin{itemize}
    \item Establecer una topología confiable del espacio de datasets, donde datasets cercanos comparten
    configuraciones de hiperparámetros similares.
    \item Reducir costos computacionales al usar configuraciones de datasets ``vecinos'' en nuevos 
    datasets.
    \item Obtener información interpretable sobre cuándo un algoritmo funciona bien, proporcionando 
    intuición sobre la naturaleza del problema.
\end{itemize}


\paragraph{Modelo 1: MetaFeatX}  
MetaFeatX es un sistema de meta-learning que aprende automáticamente meta-features representativas de los datasets para guiar la selección de algoritmos y configuraciones de hiperparámetros. La idea central es construir un \textbf{embedding} de los datasets que capture la relación entre sus características y los hiperparámetros que producen los mejores resultados. 

Su funcionamiento se basa en:

\begin{enumerate}
    \item Aprender nuevas meta-features mediante un procedimiento de \textbf{Transporte Óptimo}, alineando las meta-features manuales con la distribución de configuraciones óptimas.  
    \item Estimar la \textbf{topología} del espacio de datasets y la \textbf{dimensionalidad intrínseca} del espacio de problemas para cada algoritmo o pipeline.  
    \item Identificar los datasets más \textbf{similares} a la tarea actual, generando un ranking de algoritmos prometedores.
\end{enumerate}

\paragraph{Modelo 2: Transfer-Learning para Hiperparámetros}  
Este segundo modelo recibe el ranking de algoritmos de MetaFeatX y predice los \textbf{valores de hiperparámetros} más prometedores para cada algoritmo, basándose en los datasets vecinos en el embedding aprendido. Esto permite una inicialización eficiente de AutoML y evita el costo computacional de explorar todas las configuraciones posibles desde cero.

\paragraph{Conexión con AutoML}  
La combinación de ambos modelos permite realizar tareas típicas de AutoML de manera más eficiente:

\begin{itemize}
    \item Selección de algoritmo: basada en la similitud de datasets en el embedding de MetaFeatX.  
    \item Optimización de hiperparámetros: predicha por el modelo de transfer-learning usando vecinos.  
\end{itemize}

En resumen, este enfoque de meta-learning permite reducir la fase de \textit{cold-start}, mejorar la eficiencia de AutoML, y proporcionar interpretabilidad sobre cuándo y por qué un algoritmo funciona bien para un dataset específico.


%-------------------------------------------------
\newpage
\section{MetaFeatX}
\label{sec:metafeatx}


Obtener el máximo rendimiento de un portafolio de algoritmos para una instancia de problema específica 
se reconoce como un cuello de botella importante en dominios que van desde la Programación por 
Restricciones y Satisfacibilidad hasta el Aprendizaje Automático.
Los enfoques iniciales investigaron el uso de modelos de rendimiento generales \citep{rice1976}, 
que estiman a priori el desempeño de cualquier algoritmo sobre cualquier instancia de problema, 
donde cada instancia se describe mediante un vector de \emph{meta-features}, y el modelo 
de rendimiento se aprende en este espacio de meta-features.

En el contexto del Aprendizaje Automático supervisado, muchas meta-features han sido diseñadas 
manualmente para describir datasets.
Tras varios desafíos internacionales de AutoML, destinados a automatizar la selección y ajuste de 
pipelines de ML \citep{hutter2019,guyon2019}, se ha observado que un modelo de rendimiento general y preciso 
difícilmente puede basarse únicamente en estas meta-features \citep{misir2017}. Por ejemplo, el 
AutoSkLearn \citep{feurer2015} se basa en optimización bayesiana y aprende iterativamente un modelo de rendimiento específico para cada dataset; PMF \citep{fusi2018} utiliza un enfoque probabilístico de filtrado colaborativo, y OBOE \citep{yang2019} combina filtrado colaborativo con aprendizaje activo.

El enfoque \textbf{MetaFeatX} se inspira en el trabajo de Rakotoarison et al. \citep{rakotoarison2022}, que propone aprender meta-features capaces de capturar la topología del espacio de datasets en relación con el desempeño de un algoritmo de ML. MetaFeatX considera dos representaciones de los datasets: la básica, compuesta por 135 meta-features diseñadas manualmente, y la objetivo, que representa un dataset mediante la distribución de configuraciones de hiperparámetros de \(A\) que producen los mejores rendimientos. Para alinear ambas representaciones se utiliza Transporte Óptimo, de modo que la distancia euclidiana entre las meta-features aprendidas imite la distancia Wasserstein-Gromov sobre la representación objetivo \citep{cuturi2013,peyre2019,memoli2011}. Este procedimiento permite derivar meta-features que pueden ser computadas desde cero para nuevos datasets, sin necesidad de un arranque en frío como en \cite{yang2019,fusi2018}.

Entre los aspectos más relevantes de MetaFeatX se destacan: 
\begin{enumerate}
    \item Las meta-features definen una topología eficiente del espacio de datasets, útil para identificar regiones prometedoras de hiperparámetros.  
    \item Pueden ser empleadas como espacio de representación para inicializar otros métodos de AutoML o transfer-learning.  
    \item Permiten estimar la dimensionalidad intrínseca del espacio de datasets respecto a un algoritmo, lo que proporciona información sobre la complejidad de la tarea. Por ejemplo, se puede comparar la dimensión intrínseca de OpenML CC-18 respecto a AutoSkLearn, SVM , o Random Forest .
\end{enumerate}

Esta sección se centra en presentar los aspectos más importantes del trabajo de Rakotoarison et al. (2022) y su relevancia para la construcción de meta-features en tareas de AutoML.


\subsection{AutoML y Meta-Features}
\label{sec:automl}
Las meta-features son estadísticas que describen datasets supervisados y se han diseñado manualmente considerando información descriptiva, teoría de la información, estructura geométrica y \emph{landmarking} (por ejemplo, desempeño de clasificadores simples). En dominios relacionados, como Satisfacibilidad (SAT) o Programación por Restricciones (CP), también existen meta-features manuales.  

En AutoML, estas meta-features se utilizan principalmente para inicializar la búsqueda de optimización \citep{feurer2015}, pero no siempre garantizan un modelo de rendimiento preciso \citep{misir2017}. Por ello, se han explorado enfoques de meta-features aprendidas, usando modelos de rendimiento complejos \citep{hazan2018} o redes neuronales que representan cada dataset como función de su distribución. Sin embargo, estas redes requieren grandes cantidades de datos, mientras que los benchmarks de AutoML incluyen relativamente pocos datasets. Esto motiva métodos como MetaFeatX, que construyen representaciones efectivas basándose en meta-features existentes.

\subsection{Transporte Óptimo}
\label{sec:ot}
MetaFeatX se apoya en el Transporte Óptimo (OT) para medir la similitud entre datasets en dos representaciones: la básica (meta-features existentes) y la objetivo (rendimiento óptimo de configuraciones de hiperparámetros).  

Sea \((\Omega_x, d_x)\) y \((\Omega_y, d_y)\) espacios métricos compactos con distribuciones \(x\) y \(y\). El conjunto de distribuciones con márgenes \(x\) y \(y\) se denota \(\Gamma(x,y)\), y \(c: \Omega_x \times \Omega_y \to \mathbb{R}^+\) es la función de costo de transporte.  

El problema de OT busca una distribución \(\gamma \in \Gamma(x,y)\) que minimice el costo esperado \citep{peyre2019}:
\begin{equation}
    d_W^q(x,y) = \left( \min_{\gamma \in \Gamma(x,y)} \mathbb{E}_{(x,y)\sim\gamma} [c^q(x,y)] \right)^{1/q},
\end{equation}
definiendo la distancia de Wasserstein de orden \(q\).

La distancia \textbf{Gromov-Wasserstein (GW)} mide qué tan bien se preservan las relaciones internas de cada dominio \citep{memoli2011}:
\begin{equation}
    d_{GW}^q(x,y) = \left( \min_{\gamma \in \Gamma(x,y)} \mathbb{E}_{(x,y),(x',y')\sim\gamma} \big| d_x(x,x') - d_y(y,y') \big|^q \right)^{1/q}.
\end{equation}

La \textbf{Fused Gromov-Wasserstein (FGW)} combina ambas  \citep{titouan2019}:
\begin{equation}
\begin{aligned}
    d_{FGW;\alpha}^q(x,y) = \min_{\gamma \in \Gamma(x,y)} & \; (1-\alpha) \underbrace{\int c^q(x,y) \, d\gamma(x,y)}_{\text{Wasserstein Loss}} \\
    & + \alpha \underbrace{\int\!\int |d_x(x,x') - d_y(y,y')|^q \, d\gamma(x,y) d\gamma(x',y')}_{\text{Gromov-Wasserstein Loss}},
\end{aligned}
\end{equation}
donde \(\alpha \in [0,1]\) controla el balance: \(\alpha=0\) corresponde a Wasserstein, \(\alpha=1\) a Gromov-Wasserstein.  

Estas distancias permiten evaluar la similitud entre datasets considerando tanto las meta-features como la estructura de rendimiento de los algoritmos, y constituyen la base de MetaFeatX para construir representaciones que conectan la información básica con la objetivo, facilitando la transferencia de conocimiento entre datasets.
Esta metodología se ha usado previamente en adaptación de dominio y transfer learning \citep{Courty_2017_NeurIPS,AlvarezMelis_Fusi_2020} y en la consistencia de espacios latentes de autoencoders \citep{Xu_2020_CVPR,Nguyen_2020_ICML}, y constituye la base de MetaFeatX para vincular la información básica con la objetivo.

\subsection{Principio de MetaFeatX y Argumentación del Benchmark}
MetaFeatX busca aprender nuevas meta-features para algoritmos de ML a partir de meta-features básicas y representaciones objetivo, siguiendo un enfoque basado en Transporte Óptimo.  

Cada dataset se puede representar de dos formas:
\begin{enumerate}
    \item \textbf{Representación básica:} Un vector de las $D$ meta-features manualmente diseñadas, fácil de calcular para cualquier dataset.
    \item \textbf{Representación objetivo:} La distribución de configuraciones de hiperparámetros que producen los mejores resultados para un dataset, disponible solo para un subconjunto de datasets del benchmark.
\end{enumerate}

La idea de MetaFeatX es construir un \emph{puente} entre estas dos representaciones. Para ello:
\begin{itemize}
    \item Se proyecta la representación objetivo en un espacio de dimensión más baja $d$ mediante un método que preserve distancias, como Multi-Dimensional Scaling (MDS). Esto produce vectores $u_i \in \mathbb{R}^d$ para cada dataset \citep{cox2001}
    \item Se aprende un mapeo $\psi: \mathbb{R}^D \to \mathbb{R}^d$ que transforma la representación básica al espacio proyectado, de manera que la distancia euclidiana entre $\psi(x_i)$ refleje la topología de los $u_i$, usando la distancia \textbf{Fused Gromov-Wasserstein}.
\end{itemize}

El resultado de este mapeo son las \textbf{meta-features MetaFeatX}, que:
\begin{itemize}
    \item Se pueden calcular de manera económica a partir de las meta-features básicas.
    \item Definen una distancia euclidiana que refleja la proximidad de datasets en términos de desempeño de hiperparámetros, facilitando tareas de AutoML como inicialización de optimización o transferencia de conocimiento.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{pca2.png} % cambia la ruta según donde esté tu imagen
    \caption{
        Configuraciones principales de los datasets A, B y C, donde B (en naranja) (respecto a C, en verde) es el vecino más cercano de A con respecto a la representación objetivo.
    }
    \label{fig:metabu_top_configurations}
\end{figure}


\textbf{Argumentación del benchmark.}

Como los benchmarks de AutoML (por ejemplo, OpenML CC-18) contienen relativamente pocos datasets etiquetados con representaciones objetivo, existe riesgo de sobreajuste al aprender las meta-features.  
MetaFeatX aborda este problema mediante un procedimiento de \emph{bootstrap} \citep{efron1979}, que genera datasets adicionales a partir de los existentes para densificar el espacio de meta-features.  
Este procedimiento puede incluir la adición de ruido siguiendo una distribución gaussiana, lo que permite explorar más exhaustivamente el espacio de meta-features y mejorar la generalización del modelo.

\subsection{Algoritmo MetaFeatX}

El algoritmo MetaFeatX se entrena sobre $p = 1000 \times n$ datasets de entrenamiento del benchmark, previamente aumentados mediante \emph{bootstrap} (ver sección anterior y Apéndice B). Las meta-features de MetaFeatX se construyen en un ``procedimiento de tres pasos'' ilustrado en la Figura~\ref{fig:metabu_pipeline}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{illustration_metabu.png} % ajusta la ruta de la imagen
    \caption{De las meta-features básicas a las MetaFeatX usando Fused Gromov-Wasserstein. Representaciones básicas (círculos), MetaFeatX (cuadrados) y representaciones objetivo (subgráfico izquierdo). Datasets vecinos en el espacio objetivo mantienen el mismo color en todos los subgráficos.}
    \label{fig:metabu_pipeline}
\end{figure}

\paragraph{Paso 1: Representación objetivo y distancia de Wasserstein.}  
Para cada dataset de entrenamiento $i$, se define $\Theta_i \subset \mathcal{T}$ como el conjunto de configuraciones de hiperparámetros cuyo desempeño se encuentra en el top-L de configuraciones conocidas ($L = 20$ en los experimentos). La \emph{representación objetivo} $z_i$ se define como la distribución discreta con soporte $\Theta_i$.  
La distancia entre datasets se mide mediante la ``distancia 1-Wasserstein'':
\begin{equation}
    d_W^1(z_i, z_j) = \min_{\gamma \in \Gamma(z_i, z_j)} \mathbb{E}_{(x,y)\sim\gamma}[c(x,y)],
\end{equation}
donde $c$ es el costo de transporte Euclidiano en el espacio de configuraciones.

\paragraph{Paso 2: Proyección de la representación objetivo en $\mathbb{R}^d$.}  
La representación $z_i$ se proyecta en $\mathbb{R}^d$, donde $d$ se estima usando una medida de dimensionalidad intrínseca (ver más abajo). Se utiliza ``Multi-Dimensional Scaling (MDS)'' para que la distancia euclidiana entre las proyecciones $u_i$ y $u_j$ aproxime la distancia 1-Wasserstein:
\begin{equation}
    d(u_i, u_j) \approx d_W^1(z_i, z_j).
\end{equation}  
Estas proyecciones $u_i$ se definen hasta una isometría, pero preservan la topología relativa de los datasets en el espacio objetivo.

\paragraph{Paso 3: Aprendizaje de las MetaFeatX.}  
Se define la distribución discreta uniforme sobre las representaciones básicas:
\begin{equation}
    x = \frac{1}{p} \sum_{i=1}^{p} \delta_{x_i},
\end{equation}
y sobre las proyecciones objetivo:
\begin{equation}
    u = \frac{1}{n} \sum_{i=1}^{n} \delta_{u_i}.
\end{equation}

Las MetaFeatX se construyen encontrando un mapeo $\psi: \mathbb{R}^D \to \mathbb{R}^d$ que minimice la distancia ``Fused Gromov-Wasserstein'' entre la distribución push-forward $\psi_\# x$ y $u$:
\begin{equation}
    \psi^* = \arg \min_{\psi \in \Psi} d_{FGW;\alpha}^q(\psi_\# x, u) + \lambda \|\psi\|,
\end{equation}
donde $\lambda$ es un parámetro de regularización y $\|\psi\|$ es la norma de la función $\psi$. Se considera $\psi$ lineal para evitar sobreajuste y facilitar la interpretación respecto a las meta-features básicas.

La optimización se realiza mediante un esquema de ``bilevel optimization'' \citep{xu2020}:
\begin{itemize}
    \item \textbf{Problema interno:} minimizar $d_{FGW;\alpha}(\psi_\# x, u)$ usando un método de gradiente proximal \citep{xu2019}, refinando la matriz de transporte $\gamma$ con el ``algoritmo de Sinkhorn'' \citep{cuturi2013}.
    \item \textbf{Problema externo:} optimizar $\psi$ considerando $\gamma$ como constante, usando el optimizador ``ADAM'' \citep{kingma2015adam} con tasa de aprendizaje 0.01, $\alpha = 0.5$ y $\lambda = 0.001$.
\end{itemize}

\paragraph{Dimensión intrínseca del espacio de datasets.}  
El parámetro principal de MetaFeatX es $d$, el número de meta-features necesarias para aproximar la representación objetivo. Se estima usando un método basado en vecinos más cercanos : para cada muestra $x$, se calculan sus primeras distancias a los vecinos $x^{(1)}$ y $x^{(2)}$, y se define
\begin{equation}
    \mu(x) = \frac{d(x, x^{(2)})}{d(x, x^{(1)}) + \epsilon}.
\end{equation}  
Ordenando las muestras por $\mu(x_i)$, $d$ se obtiene como la pendiente de la aproximación lineal de la curva
\begin{equation}
    \{ (\log \mu(x_i), -\log (1 - \frac{i}{(m+1)})), 1 \le i \le m \},
\end{equation}
proporcionando una estimación garantizada de la dimensionalidad intrínseca del espacio donde habitan los datasets.

Este procedimiento asegura que las MetaFeatX reflejen la topología del espacio objetivo y puedan ser computadas de manera económica a partir de las meta-features básicas, facilitando tareas de AutoML como inicialización de optimización y transferencia de conocimiento.

%-------------------------------------------------
\section{Transfer-Learning de Hiperparámetros}
\label{sec:transfer}

%-------------------------------------------------
\section{Experimentación}
\label{sec:experimentation}

Esta sección presenta la evaluación experimental de los enfoques estudiados en este trabajo, organizada de manera progresiva y coherente con los objetivos del proyecto.  
Dado que los fundamentos teóricos y algorítmicos de los modelos han sido descritos en secciones anteriores, el énfasis aquí se pone exclusivamente en el análisis empírico de su comportamiento y en la validación de su utilidad práctica para AutoML.

En primer lugar, se reproducen y analizan los experimentos asociados al modelo \textbf{MetaFeatX}, siguiendo de cerca la metodología y los criterios de evaluación propuestos en el trabajo original que inspira este proyecto. El objetivo de esta etapa es verificar que las meta-features aprendidas capturan adecuadamente la estructura del espacio de datasets y que inducen una noción de similitud alineada con el rendimiento de los algoritmos y sus configuraciones de hiperparámetros.

A continuación, se presentan los \textbf{experimentos propios del equipo}, diseñados para estudiar el comportamiento de MetaFeatX en escenarios adicionales y para analizar empíricamente propiedades que no se exploran en profundidad en el trabajo original. Estos experimentos permiten evaluar la robustez del método, su sensibilidad a distintos parámetros y su impacto en tareas prácticas de muestreo e inicialización en AutoML.

Finalmente, se evalúa el modelo de \textbf{transfer learning para la predicción de hiperparámetros}, que utiliza las representaciones aprendidas por MetaFeatX como mecanismo de transferencia de conocimiento entre datasets. En esta etapa se analiza cómo la información estructural capturada por las meta-features puede ser explotada para inicializar de manera eficiente la búsqueda de configuraciones prometedoras, reduciendo el costo computacional asociado a la fase de \emph{cold-start} en AutoML.

Esta organización permite separar claramente la validación del enfoque inspirado en la literatura, la contribución experimental propia del equipo y la evaluación del modelo de transferencia desarrollado, ofreciendo una visión completa y estructurada de los resultados obtenidos.

\subsection{Configuraci\'on experimental}

El banco de pruebas es la suite OpenML CC-18 de clasificaci\'on tabular, con 72 conjuntos de datos, de los cuales 64 disponen de informaci\'on suficiente para construir la representaci\'on objetivo a partir de configuraciones evaluadas.  Para evitar sobreajuste dada la escasez de conjuntos de datos, los autores ampl\'ian la colecci\'on mediante \emph{bootstrap}: para cada conjunto de datos original se generan 1000 versiones remuestreadas con reemplazo, calculando de nuevo las meta-caracter\'isticas b\'asicas pero heredando la misma representaci\'on objetivo, lo que densifica el espacio de meta-features sin distorsionar la estructura de rendimiento.

Dado que este número de conjuntos de datos es relativamente reducido para el aprendizaje de un mapeo entre meta-features básicas y representaciones objetivo, existe un riesgo significativo de \textbf{sobreajuste} y de una mala generalización del modelo aprendido. Para mitigar este problema, se amplía la colección de datasets mediante un procedimiento de \emph{bootstrap}: para cada conjunto de datos original se generan 1000 versiones remuestreadas con reemplazo, recalculando las meta-características básicas pero heredando la misma representación objetivo.

Este procedimiento permite \textbf{densificar el espacio de meta-features} y estabilizar el aprendizaje del mapeo inducido por el Transporte Óptimo, sin introducir información artificial sobre el rendimiento de las configuraciones. De esta forma, se preserva la estructura topológica del espacio objetivo mientras se reduce la varianza del modelo y se mejora su capacidad de generalización a nuevos conjuntos de datos.

\paragraph{Hiperparámetros del modelo} Para estos experimentos, el modelo fue configurado con los siguientes valores:  
\begin{itemize}
    \item $\alpha = 0.5$
    \item $\lambda_{\text{reg}} = 1\times 10^{-3}$
    \item $\text{learning\_rate} = 1\times 10^{-2}$
    \item $\text{seed} = 42$
\end{itemize}

\subsection{Tareas experimentales}

La experimentaci\'on en este trabajo se organiza en torno a tres objetivos principales: Validar si las meta-caracter\'isticas aprendidas por MetaFeatX capturan adecuadamente la topolog\'ia ``verdadera'' inducida por las configuraciones de hiperpar\'ametros; evaluar su utilidad pr\'actica para AutoML, tanto sin como con modelo de rendimiento; y analizar la sensibilidad del m\'etodo y extraer informaci\'on sobre la estructura del espacio de conjuntos de datos y los ``nichos'' de distintos algoritmos.  La experimentaci\'on se fundamenta en la idea de que unas buenas meta-caracter\'isticas deben inducir una m\'etrica sobre el espacio de conjuntos de datos tal que la cercan\'ia entre conjuntos de datos signifique tambi\'en similitud en los conjuntos de configuraciones de hiperpar\'ametros que proporcionan buen rendimiento.

Esta noci\'on se formaliza mediante representaciones objetivo de cada conjunto de datos como distribuciones discretas sobre configuraciones de hiperpar\'ametros de alto rendimiento, y distancias de Wasserstein y Gromov-Wasserstein entre dichas distribuciones.  Desde este punto de vista, el prop\'osito de la experimentaci\'on es triple: (1) comprobar si la distancia eucl\'idea en el espacio de meta-caracter\'isticas MetaFeatX aproxima bien la distancia de Wasserstein entre representaciones objetivo; (2) verificar si esta m\'etrica permite dise\~nar estrategias de muestreo inicial de configuraciones m\'as eficaces que los meta-features manuales cl\'asicos (AutoSklearn, Landmark, SCOT) o el muestreo uniforme; y (3) estudiar, a partir de las meta-caracter\'isticas aprendidas, la dimensi\'on intr\'inseca del espacio de conjuntos de datos y la importancia relativa de distintas meta-caracter\'isticas humanas para varios algoritmos.

\paragraph{Configuraci\'on de las tareas: }

Sobre las base de la configuraciones experimentales anteriormente descrita, se definen tres tareas experimentales, todas validadas con un esquema \emph{leave-one-out} sobre los 64 conjuntos de datos con representaci\'on objetivo.  La primera tarea eval\'ua la capacidad de capturar la topolog\'ia objetivo; la segunda estudia el comportamiento de AutoML sin modelo de rendimiento expl\'icito, en un escenario de inicializaci\'on basada en vecindario; y la tercera analiza el uso de las meta-caracter\'isticas como mecanismo de inicializaci\'on para sistemas AutoML con modelo de rendimiento, concretamente AutoSklearn y Probabilistic Matrix Factorization (PMF).

\subsubsection{Tarea 1: Captura de la topolog\'ia objetivo}

En la primera tarea, el objetivo es cuantificar hasta qu\'e punto el vecindario de un conjunto de datos en el espacio de meta-caracter\'isticas MetaFeatX coincide con su vecindario real en el espacio de distribuciones de configuraciones de alto rendimiento.  Para cada conjunto de datos de prueba se calcula, por un lado, la lista ordenada de vecinos m\'as cercanos seg\'un la distancia de Wasserstein entre sus representaciones objetivo, y, por otro, la lista de vecinos seg\'un la distancia eucl\'idea en el espacio de meta-caracter\'isticas MetaFeatX y en los espacios de referencia (AutoSklearn, Landmark, SCOT).

La similitud entre ambas listas se mide mediante la m\'etrica NDCG@k (normalized discounted cumulative gain) sobre los primeros k vecinos, con valores de k entre 5 y 35, y se considera como indicador el NDCG@k medio sobre todos los conjuntos de datos de prueba.  Te\'oricamente, un valor elevado de NDCG indica que la m\'etrica inducida por las meta-caracter\'isticas preserva la estructura de vecindad que es relevante para AutoML, es decir, la inducida por el comportamiento de las configuraciones de hiperpar\'ametros sobre los distintos conjuntos de datos.

Dado que MetaFeatX aprende las meta-caracter\'isticas mediante un procedimiento estocástico, cada experimento se repite varias veces variando \emph{únicamente} la semilla aleatoria para capturar la variabilidad inherente al proceso de entrenamiento. Por el contrario, las meta-caracter\'isticas de referencia, al ser deterministas, se evalúan una sola vez. Los resultados reportados en la Figura~\ref{fig:task1} representan el promedio de NDCG@k sobre todas las repeticiones y todos los conjuntos de datos de prueba, reflejando así tanto la calidad como la estabilidad de MetaFeatX frente a la aleatoriedad del aprendizaje.

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{task1.png}
    \caption{Captura de la topología objetivo. 
    Similaridad NDCG@k entre el vecindario inducido por las meta-características y el vecindario real basado en distribuciones de configuraciones de alto rendimiento.}
    \label{fig:task1}
\end{figure}

\subsubsection{Tarea 2: AutoML sin modelo de rendimiento}

En la segunda tarea se evalúa la capacidad de MetaFeatX para guiar el muestreo inicial de configuraciones de hiperparámetros sin recurrir a un modelo de rendimiento explícito, utilizando únicamente la información de vecindario. Para cada conjunto de datos de prueba y cada conjunto de meta-características (\textit{MetaFeatX}, AutoSklearn, Landmark, SCOT), se define una distribución sobre configuraciones, denotada $\hat{z}^{mf}$, calculada como mezcla ponderada de las distribuciones objetivo de los diez vecinos más cercanos en el espacio correspondiente:

\[
\hat{z}^{mf} = \frac{1}{Z} \sum_{\ell=1}^{10} \exp(-\ell) z^\ell
\]

donde $z^\ell$ es la representación objetivo del $\ell$-ésimo vecino más cercano según la distancia euclídea en el espacio de meta-características $mf$, y $Z$ es una constante de normalización. Esta distribución se utiliza para ``muestrear iterativamente configuraciones de hiperparámetros'', entrenar los modelos y registrar el rendimiento de la mejor configuración encontrada hasta la iteración $t$.  

Para cada iteración $t$, se calcula el rango $r(t, mf)$ de cada conjunto de meta-características, que indica la posición relativa de dicho método frente a los demás (MetaFeatX, AutoSklearn, Landmark, SCOT) y frente a un muestreador uniforme de referencia (\textit{Random1x}). Este rango se asigna observando cuál método ha encontrado la mejor configuración de hiperparámetros hasta la iteración $t$: el método con mejor rendimiento recibe rango 1, el siguiente rango 2, y así sucesivamente hasta 5.  

Un rango más bajo implica que la métrica es más efectiva para identificar regiones prometedoras del espacio de hiperparámetros sin entrenar previamente el modelo en el dataset de prueba. La inclusión del muestreador uniforme permite verificar que los métodos basados en meta-características aportan información útil más allá de la simple aleatoriedad.  

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{task2.png} 
    \caption{Task 2: muestreo de la distribución de configuraciones de hiperparámetros sin modelo de rendimiento. Las curvas muestran el rango promedio $r(t, mf)$ sobre todos los conjuntos de datos de prueba.}
    \label{fig:task2}
\end{figure}

\subsubsection{Tarea 3: AutoML con modelo de rendimiento}

La tercera tarea evalúa el papel de las meta-características como mecanismo de inicialización de sistemas de AutoML basados en modelos de rendimiento, específicamente AutoSklearn y PMF. En estos sistemas, la búsqueda de configuraciones es ``adaptativa'': se seleccionan iterativamente configuraciones de hiperparámetros, se observa su rendimiento y se actualiza un modelo probabilístico que guía las decisiones de muestreo posteriores.

En esta tarea, las meta-características se emplean para seleccionar un conjunto inicial de configuraciones prometedoras. Para AutoSklearn, se consideran las mejores configuraciones de los diez vecinos más cercanos del dataset de prueba según la distancia en el espacio de meta-características, y se evalúan en el conjunto de datos objetivo para alimentar el modelo de rendimiento inicial, sobre el cual se ejecuta la optimización bayesiana. En PMF, se realiza un procedimiento análogo utilizando cinco vecinos, cuyos mejores resultados se usan para rellenar parcialmente la fila correspondiente en la matriz de rendimientos y así inicializar su representación latente.

A partir de esta fase de inicialización, el sistema continúa su búsqueda estándar. El rendimiento se resume mediante ``curvas de rango'' en función del número de iteraciones, comparando las versiones originales de AutoSklearn y PMF con sus variantes híbridas MetaFeatX+AutoSklearn y MetaFeatX+PMF. También se incluyen muestreadores uniformes reforzados (Random2× y Random4×), que seleccionan el mejor de varios candidatos aleatorios por iteración, como referencia de exploración aleatoria.  

Esta tarea refleja la diferencia entre ``Exploitation y Exploration'': mientras que en Task 2 se buscaba identificar rápidamente configuraciones prometedoras (Exploitation), en Task 3 se trata de inicializar la búsqueda en una ``región diversa y de calidad'' del espacio de hiperparámetros, de modo que la optimización adaptativa pueda descubrir configuraciones aún mejores a lo largo del tiempo.

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{task3.png}
    \caption{AutoML con modelo de rendimiento. Curvas de rango $r(t, mf)$ que comparan las versiones originales de AutoSklearn y PMF con sus variantes híbridas inicializadas con MetaFeatX y con muestreadores uniformes. Un rango menor indica mejor desempeño relativo.}
    \label{fig:task3}
\end{figure}


\subsubsection{Algoritmos, hiperpar\'ametros y protocolo}

Los experimentos se realizan para tres algoritmos de aprendizaje supervisado (AdaBoost, RandomForest y SVM, usando implementaciones de \texttt{scikit-learn}) y dos pipelines de AutoML (AutoSklearn y PMF).  Para cada algoritmo se define un espacio de hiperpar\'ametros de dimensi\'on moderada, incluyendo por ejemplo el n\'umero de \'arboles, la profundidad m\'axima y los criterios de divisi\'on en RandomForest, as\'i como par\'ametros como $C$, tipo de \emph{kernel}, grado y $\gamma$ en el caso de SVM, y un conjunto amplio de opciones de preprocesamiento y clasificaci\'on en AutoSklearn.

Las representaciones objetivo de los conjuntos de datos se construyen a partir del conjunto de configuraciones con mejor rendimiento disponible: las veinte mejores sobre decenas de miles de configuraciones evaluadas en OpenML para AdaBoost, RandomForest y SVM, quinientas configuraciones por conjunto de datos para AutoSklearn y las veinte mejores entradas de la matriz de filtrado colaborativo en el caso de PMF.  El entrenamiento y evaluaci\'on de cada configuraci\'on se realiza sobre las particiones de entrenamiento, validaci\'on y prueba provistas por OpenML, recurriendo a validaci\'on cruzada de cinco pliegues para estimar el rendimiento.

En todas las tareas, los indicadores se calculan mediante el esquema \emph{leave-one-out}: para cada conjunto de datos que dispone de representaci\'on objetivo, se entrena MetaFeatX sobre el resto de conjuntos de datos y sus versiones de bootstrap, y se eval\'ua sobre el conjunto excluido; adicionalmente, los ocho conjuntos de datos sin representaci\'on objetivo se incorporan como casos de prueba en las tareas de muestreo de configuraciones, donde solo es necesario observar el rendimiento y no la estructura de la representaci\'on objetivo.

\subsubsection{Resultados y an\'alisis}

En la Tarea 1, las curvas de NDCG@k muestran que la m\'etrica inducida por MetaFeatX se alinea mejor que las referencias con la topolog\'ia objetivo, especialmente a medida que aumenta el valor de $k$.  Aunque la varianza es mayor debido a la naturaleza entrenable de MetaFeatX, los resultados indican que supera de forma significativa a las meta-caracter\'isticas de AutoSklearn, Landmark y SCOT para todos los valores de $k$ y para todos los espacios de hiperpar\'ametros considerados, lo que respalda la idea de que las meta-caracter\'isticas aprendidas capturan de forma m\'as fiel la estructura relevante para AutoML.

En la Tarea 2, las curvas de rango evidencian una mejora sustancial en la calidad del muestreo de configuraciones.  Para RandomForest, las meta-caracter\'isticas SCOT resultan competitivas en las primeras iteraciones, pero MetaFeatX pasa a dominar en fases posteriores; para AdaBoost, las meta-caracter\'isticas de AutoSklearn pueden ofrecer ligeras ventajas en las tres primeras configuraciones evaluadas, mientras que MetaFeatX se vuelve significativamente mejor a partir de ese punto; para SVM, MetaFeatX supera de forma clara y consistente a todas las alternativas a lo largo de toda la b\'usqueda.  Estos resultados indican que la topolog\'ia inducida por MetaFeatX permite explotar con mayor eficacia los vecindarios de conjuntos de datos a la hora de seleccionar regiones prometedoras del espacio de configuraciones, incluso en ausencia de un modelo de rendimiento adaptativo.

En la Tarea 3, las variantes MetaFeatX+AutoSklearn y MetaFeatX+PMF mejoran los rangos obtenidos por las versiones originales de AutoSklearn y PMF, pese a que la \'unica diferencia entre ellas reside en la fase de inicializaci\'on.  En particular, MetaFeatX+AutoSklearn sobrepasa al pipeline AutoSklearn puro a partir de un n\'umero moderado de iteraciones, y MetaFeatX+PMF consigue superar al muestreo uniforme reforzado que elige el mejor de cuatro configuraciones aleatorias a partir de aproximadamente la d\'ecima iteraci\'on.  Este comportamiento sugiere que las meta-caracter\'isticas de MetaFeatX no solo favorecen la explotaci\'on, al identificar una buena regi\'on inicial del espacio de configuraciones, sino que tambi\'en facilitan una exploraci\'on eficaz cuando se combinan con modelos de rendimiento que se actualizan de manera incremental.

\subsubsection{Sensibilidad e interpretabilidad}

El análisis de sensibilidad se centra en los dos hiperparámetros propios de MetaFeatX: $\alpha$, que pondera las componentes Wasserstein y Gromov-Wasserstein en la distancia FGW, y $\lambda$, que controla el peso de la regularización L1 sobre la transformación lineal $\psi$. La sensibilidad se evalúa sobre Task 1, inspeccionando la diferencia
\[
\text{NDCG@10(MetaFeatX)} - \text{NDCG@10(AutoSklearn)}
\]
para $\alpha \in \{0.1, 0.3, 0.5, 0.7, 0.99\}$ y $\lambda \in \{10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}\}$. Los resultados, mostrados en la Figura~\ref{fig:variabilidad}, indican que la diferencia es positiva en todo el dominio considerado y estadísticamente significativa según un test t de Student con $p<0.05$.  

Se observa además que MetaFeatX presenta baja sensibilidad frente a $\lambda$ siempre que esta sea suficientemente pequeña ($\lambda \leq 10^{-3}$). En este rango, también se aprecia baja sensibilidad respecto a $\alpha$ en el intervalo $[0.3,0.7]$. Esto confirma la importancia de considerar ambas distancias: descartar la componente Wasserstein ($\alpha \leq 0.1$) o la Gromov-Wasserstein ($\alpha \geq 0.99$) degrada significativamente el rendimiento, mientras que los valores intermedios producen resultados estables y robustos.

En cuanto a la dimensión intrínseca del espacio de conjuntos de datos, estimada a partir de la distribución de distancias de Wasserstein de orden uno entre representaciones objetivo, se obtienen valores aproximados de 6 para AutoSklearn, 8 para AdaBoost, 9 para RandomForest y 14 para SVM. Un análisis adicional de estabilidad muestra cómo esta dimensión crece al incorporar más conjuntos de datos, ofreciendo una interpretación cuantitativa de la complejidad del problema de AutoML desde la perspectiva de cada algoritmo: cuanto más variada es la respuesta de un algoritmo a distintos conjuntos de datos, mayor es la dimensión intrínseca asociada.

Finalmente, la interpretabilidad de las meta-características se analiza proyectando las representaciones aprendidas mediante análisis de componentes principales (PCA) y midiendo la importancia de cada meta-característica manual como contribución a la primera componente principal. Representando cada meta-característica como un punto en el plano definido por sus importancias para dos algoritmos distintos, se obtiene una visión de qué propiedades de los datos son compartidas o específicas de cada algoritmo. Por ejemplo, el índice de Dunn y las medidas de importancia de atributos resultan relevantes tanto para RandomForest como para AdaBoost, mientras que la proporción de instancias con valores perdidos y el desbalanceo de clases afectan más a AdaBoost, y la dispersidad y la asimetría de los atributos tienen mayor peso en SVM que en RandomForest.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{variabilidades.png}
    \caption{Sensibilidad de MetaFeatX frente a los hiperparámetros $\alpha$ y $\lambda$, medida como la diferencia NDCG@10 respecto a AutoSklearn. Valores más oscuros indican mejor rendimiento.}
    \label{fig:variabilidad}
\end{figure}

\subsection{Experimentación adicional y análisis propio}


%-------------------------------------------------
\section{Discusión}

%-------------------------------------------------
\section{Conclusiones}


%-------------------------------------------------
\begin{center}
{\LARGE \textbf{Anexos}}\\[0.8em]
{\Large Material Suplementario}\\[2.9em]
\end{center}

El material suplementario incluye detalles adicionales sobre:

\begin{itemize}
    \item La ampliación del benchmark OpenML (\hyperref[appendix:A]{Apéndice A})
    \item Pseudo-código del algoritmo (\hyperref[appendix:B]{Apéndice B})
    \item Configuración experimental, indicadores de desempeño y procedimiento de validación (\hyperref[appendix:C]{Apéndice C})
    \item Espacio de configuración de hiperparámetros (\hyperref[appendix:D]{Apéndice D})
    \item Lista de meta-features básicas y conjuntos de meta-features de referencia (\hyperref[appendix:E]{Apéndice E})
    \item Detalles sobre el tiempo computacional (\hyperref[appendix:F]{Apéndice F})
    \item Información sobre el problema AutoML proporcionada por el enfoque: dimensionalidad intrínseca (\hyperref[appendix:G1]{Apéndice G.1}) y visualización de los nichos de los algoritmos ML considerados (\hyperref[appendix:G2]{Apéndice G.2})
    \item Resultados detallados con desviación estándar en las tres tareas (\hyperref[appendix:H]{Apéndice H})
    \item Comparación par a par de MetaFeatX con meta-features de referencia (\hyperref[appendix:I]{Apéndice I})
    \item Análisis de sensibilidad de la dimensión $d$ (\hyperref[appendix:J]{Apéndice J})
    \item Curvas de desempeño de la Tarea 2 (\hyperref[appendix:K]{Apéndice K})
\end{itemize}

\newpage
\appendix

\section*{Ampliación del benchmark OpenML}
\label{appendix:A}
\lipsum[1] % ejemplo de contenido

\section*{Pseudo-código del algoritmo}
\label{appendix:B}
\lipsum[2]

\section*{Configuración experimental y procedimiento de validación}
\label{appendix:C}
\lipsum[3] 

\newpage
\section*{Espacios de Configuración de Hiperparámetros}
\label{appendix:D}

Las configuraciones de hiperparámetros utilizadas para Adaboost, Random Forest y SVM, así como sus 
rangos, se detallan en la Tabla~\ref{tab:hp_ranges}. Para AutoSkLearn, solo se incluyó la lista de 
hiperparámetros considerados; sus rangos están detallados en \citep{feurer2015autosklearn}. El espacio 
de hiperparámetros usado en PMF es el mismo que en AutoSkLearn. La implementación de MetaFeatX
utiliza la librería \texttt{ConfigSpace} \citep{lindauer2019configspace} para gestionar los 
hiperparámetros.

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{Clasificador} & \textbf{Hiper-Par\'ametro (HP)} & \textbf{Rango} \\
\hline
\multirow{3}{*}{Adaboost} 
& imputation & mean, median, most frequent \\
& n\_estimators & [50, 500] \\
& algorithm & SAMME, SAMME.R \\
& max\_depth & [1, 10] \\
& learning\_rate & [0.01 , 2.0] \\
\hline
\multirow{7}{*}{Random Forest (RF)} 
& imputation & mean, median, most frequent \\
& criterion & gini, entropy \\
& max\_features & (0, 1] \\
& min\_samples\_split & [2, 20] \\
& min\_samples\_leaf & [1, 20] \\
& bootstrap & True, False \\
\hline
\multirow{8}{*}{SVM} 
& imputation & mean, median, most frequent \\
& C & [0.03125, 32768] \\
& kernel & rbf, poly, sigmoid \\
& degree & [1, 5] \\
& gamma & $[3.0517578125\times 10^{-5}, 8]$ \\
& coef0 & [-1, 1] \\
& shrinking & True, False \\
& tol & $[10^{-5}, 10^{-1}]$ \\
& max\_iter & -1 \\
\hline
\end{tabular}
\caption{Rangos de hiperparámetros para Adaboost, Random Forest y SVM.}
\label{tab:hp_ranges}
\end{table}

\newpage

\begin{table}[h!]
\centering
\scriptsize
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.1}

\begin{tabular}{p{4cm} p{10cm}}
\toprule
\textbf{Método} & \textbf{Parámetros} \\
\midrule

balancing & strategy \\

adaboost & learning\_rate, max\_depth, n\_estimators \\

bernoulli\_nb & fit\_prior \\

decision\_tree & max\_depth\_factor, max\_features, max\_leaf\_nodes,
min\_impurity\_decrease, min\_samples\_leaf, min\_samples\_split,
min\_weight\_fraction\_leaf \\

extra\_trees & criterion, max\_depth, max\_features, max\_leaf\_nodes,
min\_impurity\_decrease, min\_samples\_leaf, min\_samples\_split,
min\_weight\_fraction\_leaf \\

gradient\_boosting & l2\_regularization, learning\_rate, loss, max\_bins,
max\_depth, max\_leaf\_nodes, min\_samples\_leaf, scoring, tol,
n\_iter\_no\_change, validation\_fraction \\

k\_nearest\_neighbors & p, weights \\

lda & tol, shrinkage\_factor \\

liblinear\_svc & dual, fit\_intercept, intercept\_scaling, loss,
multi\_class, penalty, tol \\

libsvm\_svc & gamma, kernel, max\_iter, shrinking, tol, coef0, degree \\

mlp & alpha, batch\_size, beta\_1, beta\_2, early\_stopping, epsilon,
hidden\_layer\_depth, learning\_rate\_init, n\_iter\_no\_change,
num\_nodes\_per\_layer, shuffle, solver, tol, validation\_fraction \\

multinomial\_nb & fit\_prior \\

passive\_aggressive & average, fit\_intercept, loss, tol \\

qda & reg\_param \\

random\_forest & criterion, max\_depth, max\_features, max\_leaf\_nodes,
min\_impurity\_decrease, min\_samples\_leaf, min\_samples\_split,
min\_weight\_fraction\_leaf \\

sgd & average, fit\_intercept, learning\_rate, loss, penalty, tol,
epsilon, eta0, l1\_ratio, power\_t \\

extra\_trees\_preproc\_for\_classification & criterion, max\_depth,
max\_features, max\_leaf\_nodes, min\_impurity\_decrease,
min\_samples\_leaf, min\_samples\_split, min\_weight\_fraction\_leaf,
n\_estimators \\

fast\_ica & fun, whiten, n\_components \\

feature\_agglomeration & linkage, n\_clusters, pooling\_func \\

kernel\_pca & n\_components, coef0, degree, gamma \\

kitchen\_sinks & n\_components \\

liblinear\_svc\_preprocessor & dual, fit\_intercept, intercept\_scaling,
loss, multi\_class, penalty, tol \\

nystroem\_sampler & n\_components, coef0, degree, gamma \\

pca & whiten \\

polynomial & include\_bias, interaction\_only \\

random\_trees\_embedding & max\_depth, max\_leaf\_nodes, min\_samples\_leaf,
min\_samples\_split, min\_weight\_fraction\_leaf, n\_estimators \\

select\_percentile\_classification & score\_func \\

select\_rates\_classification & score\_func, mode \\

\bottomrule
\end{tabular}

\caption{Lista de hiperparámetros considerados en la pipeline de AutoSkLearn.}
\label{tab:autosklearn_hp}
\end{table}


\newpage
\section*{Lista de meta-características básicas y conjuntos de meta-características de referencia}
\label{appendix:E}

La lista de meta-features utilizadas en los experimentos se detalla en las Tablas 3 y 4.
Las meta-features se extraen con PyMFE \citep{alcobaca2020pymfe}, excepto las meta-features de 
AutoSkLearn, SCOT y Landmark, que se calculan a partir de la biblioteca AutoSkLearn.

\begin{center}
\scriptsize
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1.0}

\begin{longtable}{p{6.5cm} >{\raggedright\arraybackslash}p{8cm}}
\caption{Lista de meta-features utilizadas en los experimentos (1/2).} \label{tab:metafeatures_1} \\

\toprule
\textbf{Meta-feature} & \textbf{Descripción} \\
\midrule
\endfirsthead

\toprule
\textbf{Meta-feature} & \textbf{Descripción}\\
\midrule
\endhead

\midrule
\multicolumn{2}{r}{\textit{Continuación en la siguiente página}} \\
\endfoot

\bottomrule
\endlastfoot

best\_node & Rendimiento del mejor nodo individual de un árbol de decisión. \\
elite\_nn & Rendimiento del clasificador Elite Nearest Neighbor. \\
linear\_discr & Rendimiento del clasificador Linear Discriminant. \\
naive\_bayes & Rendimiento del clasificador Naive Bayes. \\
one\_nn & Rendimiento del clasificador 1-Nearest Neighbor. \\
random\_node & Nodo aleatorio del árbol de decisión. \\
worst\_node & Nodo menos informativo del árbol de decisión. \\
one\_itemset & Meta-feature basada en un solo itemset. \\
two\_itemset & Meta-feature basada en dos itemsets. \\
c1 & Entropía de las proporciones de clase. \\
c2 & Razón de desbalance de clases. \\
cls\_coef & Coeficiente de clustering. \\
density & Densidad promedio del grafo. \\
f1 & Máxima razón discriminante de Fisher. \\
f1v & Razón discriminante direccional de Fisher. \\
f2 & Volumen de la región de solapamiento. \\
f3 & Máxima eficiencia individual de característica. \\
f4 & Eficiencia colectiva de características. \\
hubs & Hub score de la red. \\
l1 & Suma de errores por programación lineal. \\
l2 & Tasa de error OVO del clasificador lineal. \\
l3 & No linealidad de clasificador lineal. \\
lsc & Cardinalidad promedio del conjunto local. \\
n1 & Fracción de puntos borderline. \\
n2 & Razón de distancias intra/extraclase NN. \\
n3 & Tasa de error del k-NN. \\
n4 & Fracción de hiperesferas que cubren los datos. \\
t1 & Promedio de características por dimensión. \\
t2 & Promedio de dimensiones PCA por punto. \\
t3 & Ratio de dimensión PCA a dimensión original. \\
t4 & Índice de Calinski y Harabasz. \\
ch & Índice de Calinski y Harabasz. \\
int & INT index. \\
nre & Entropía relativa normalizada. \\
pb & Pearson entre matching de clase e instancias. \\
sc & Número de clusters menores a un tamaño dado. \\
sil & Valor medio de silhouette. \\
vdb & Índice de Davies-Bouldin. \\
vdu & Índice de Dunn. \\
leaves & Número de nodos hoja en el DT. \\
leaves\_branch & Tamaño de ramas del DT. \\
leaves\_corrob & Corroboración de hojas del DT. \\
leaves\_homo & Homogeneidad de hojas. \\
leaves\_per\_class & Proporción de hojas por clase. \\
nodes & Número de nodos no-hoja en el DT. \\
nodes\_per\_attr & Ratio nodos/atributos. \\
nodes\_per\_inst & Ratio nodos/no-hojas por instancia. \\
nodes\_per\_level & Ratio nodos por nivel. \\
nodes\_repeated & Nodos repetidos. \\
tree\_depth & Profundidad de árbol. \\
tree\_imbalance & Desbalance de árbol. \\
tree\_shape & Forma del árbol. \\
var\_importance & Importancia de características del DT. \\
can\_cor & Correlaciones canónicas. \\
cor & Correlación absoluta entre pares de columnas. \\
cov & Covarianza absoluta entre pares de atributos. \\
eigenvalues & Valores propios de la matriz de covarianza. \\
g\_mean & Media geométrica. \\
gravity & Distancia entre clases minoritaria y mayoritaria. \\
h\_mean & Media armónica. \\
iq\_range & Rango intercuartílico (IQR). \\
kurtosis & Curtosis. \\
lh\_trace & Lawley-Hotelling trace. \\
mad & MAD ajustada. \\
max & Máximo. \\
mean & Media. \\
median & Mediana. \\
min & Mínimo. \\
nr\_cor\_attr & Número de pares altamente correlacionados. \\
nr\_disc & Número de atributos discretos. \\
nr\_norm & Número de atributos normales. \\
nr\_outliers & Número de outliers. \\
p\_trace & Pillai’s trace. \\
range & Rango (max-min). \\
roy\_root & Raíz más grande de Roy.  \\
sd & Desviación estándar de cada atributo. \\
sd\_ratio & Prueba estadística de homogeneidad de covarianzas.  \\
skewness & Sesgo de cada atributo. \\
sparsity & Medida de sparsity (posiblemente normalizada).  \\
t\_mean & Media recortada de cada atributo.  \\
var & Varianza de cada atributo.  \\
w\_lambda & Valor de Wilks’ Lambda.  \\
attr\_conc & Coeficiente de concentración entre pares de atributos. \\
attr\_ent & Entropía de Shannon de cada atributo predictivo.  \\
class\_conc & Coeficiente de concentración entre atributo y clase. \\
class\_ent & Entropía de Shannon del atributo objetivo. \\
eq\_num\_attr & Número de atributos equivalentes para la tarea. \\
joint\_ent & Entropía conjunta entre cada atributo y la clase. \\
mut\_inf & Información mutua entre cada atributo y la clase.  \\
ns\_ratio & Medida de ruido de atributos. \\
cohesiveness & Distancia ponderada mejorada que mide densidad de distribución. \\
conceptvar & Variación del concepto estimando la variabilidad de clases. \\
impconceptvar & Variación mejorada del concepto estimando variabilidad de clases. \\
wg\_dist & Distancia ponderada de la distribución de ejemplos. \\
attr\_to\_inst & Ratio entre número de atributos y número de instancias. \\
cat\_to\_num & Ratio entre número de atributos categóricos y numéricos. \\
freq\_class & Frecuencia relativa de cada clase. \\
inst\_to\_attr & Ratio entre número de instancias y atributos. \\
nr\_attr & Número total de atributos. \\
nr\_bin & Número de atributos binarios. \\
nr\_cat & Número de atributos categóricos. \\
nr\_class & Número de clases distintas. \\
nr\_inst & Número de instancias (filas) del dataset. \\
nr\_num & Número de atributos numéricos. \\
num\_to\_cat & Número de atributos numéricos y categóricos. \\
PCASkewnessFirstPC & Sesgo de ejemplos en el primer componente principal. \\
PCAKurtosisFirstPC & Curtosis de ejemplos en el primer componente principal. \\
PCAFracOfCompFor95Per & Fracción de componentes explicando 95\% de varianza.  \\
Landmark1NN & Rendimiento del clasificador 1-NN.  \\
LandmarkRandomNodeLearner &  Rendimiento de Random Node Learner.  \\
LandmarkDecisionNodeLearner & Rendimiento de Decision Node Learner.  \\
LandmarkDecisionTree & Rendimiento de Decision Tree.  \\
LandmarkNaiveBayes & Rendimiento de Naive Bayes. \\
LandmarkLDA & Rendimiento de LDA.  \\
SkewnessSTD & Desviación estándar del sesgo de características.  \\
SkewnessMean & Media del sesgo de características. \\
SkewnessMax & Máximo del sesgo de características.  \\
SkewnessMin & Mínimo del sesgo de características.  \\
KurtosisSTD & Desviación estándar de curtosis de características.  \\
KurtosisMean & Media de curtosis de características.  \\
KurtosisMax & Máximo de curtosis de características.  \\
KurtosisMin & Mínimo de curtosis de características.  \\
SymbolsSum & Suma de símbolos de características categóricas.  \\
SymbolsSTD & Desviación estándar de símbolos de características categóricas.  \\
SymbolsMean & Media de símbolos de características categóricas.  \\
SymbolsMax & Máximo de símbolos de características categóricas. \\
SymbolsMin & Mínimo de símbolos de características categóricas.  \\
ClassProbabilitySTD & Desviación estándar de probabilidades de clase.  \\
ClassProbabilityMean & Media de probabilidades de clase.  \\
ClassProbabilityMax & Máximo de probabilidades de clase.  \\
ClassProbabilityMin & Mínimo de probabilidades de clase.  \\
InverseDatasetRatio & Inverso del ratio dataset.  \\
DatasetRatio & Ratio dataset. \\
RatioNominalToNumerical & Ratio de atributos nominales a numéricos. \\
RatioNumericalToNominal & Ratio de atributos numéricos a nominales.  \\
NumberOfCategoricalFeatures & Número de atributos categóricos. \\
NumberOfNumericFeatures &  Número de atributos numéricos.  \\
NumberOfMissingValues & Número de valores faltantes.  \\
NumberOfFeaturesWithMissingValues & Número de atributos con valores faltantes.  \\
NumberOfInstancesWithMissingValues & Número de instancias con valores faltantes.  \\
NumberOfFeatures & Número total de atributos. \\
NumberOfClasses & Número de clases.  \\
NumberOfInstances & Número de instancias.  \\
LogInverseDatasetRatio & Logaritmo del inverso del ratio dataset.  \\
LogDatasetRatio & Logaritmo del ratio dataset. \\
PercentageOfMissingValues & Porcentaje de valores faltantes.  \\
PercentageOfFeaturesWithMissingValues & Porcentaje de atributos con valores faltantes.  \\
PercentageOfInstancesWithMissingValues & Porcentaje de instancias con valores faltantes.  \\
LogNumberOfFeatures & Logaritmo del número de atributos.  \\
LogNumberOfInstances & Logaritmo del número de instancias.  \\

\end{longtable}
\end{center}



\newpage
\section*{Detalles del tiempo computacional}
\label{appendix:F}
\lipsum[6]

\section*{Perspectivas sobre el problema de AutoML}
\label{appendix:G1}
\lipsum[7]

\subsection*{Visualización de los nichos de los algoritmos de ML}
\label{appendix:G2}
\lipsum[8]

\section*{Resultados detallados en las tres tareas}
\label{appendix:H}
\lipsum[9]

\section*{Comparación por pares con las meta-características de referencia}
\label{appendix:I}
\lipsum[10]

\section*{Análisis de sensibilidad de la dimensión $d$}
\label{appendix:J}
\lipsum[11]

\section*{Curvas de rendimiento de la Tarea 2}
\label{appendix:K}
\lipsum[12]

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
