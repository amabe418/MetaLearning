\documentclass[11pt,a4paper]{article}

% =============================================================================
% PAQUETES
% =============================================================================
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[table]{xcolor}
\usepackage{colortbl}
\usepackage{hyperref}
\usepackage{float}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[margin=2.5cm]{geometry}
\usepackage{fancyhdr}
\usepackage{setspace}

% Colores para tablas
\definecolor{winner}{RGB}{200, 255, 200}
\definecolor{loser}{RGB}{255, 200, 200}
\definecolor{tie}{RGB}{255, 255, 200}

% Configuración de hyperref
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=green
}

% =============================================================================
% TÍTULO
% =============================================================================
\title{
    \textbf{Reporte de Experimentación: Few-Shot Bayesian Optimization (FSBO)} \\
    \large Análisis Comparativo con Baselines de HPO \\
    \vspace{0.5cm}
    \normalsize Proyecto Meta-Learning para Optimización de Hiperparámetros
}

\author{
    Proyecto Académico MetaLearning \\
    \texttt{transfer-learning}
}

\date{Enero 2026}

\begin{document}

\maketitle

\begin{abstract}
Este reporte presenta los resultados experimentales de la evaluación del modelo \textbf{Few-Shot Bayesian Optimization (FSBO)} implementado para optimización de hiperparámetros de algoritmos de machine learning. Se utilizó un protocolo de \textbf{5-Fold Cross-Validation sobre tareas} con múltiples semillas aleatorias para garantizar robustez estadística. Los experimentos se ejecutaron sobre 4 algoritmos de clasificación (AdaBoost, Random Forest, LibSVM\_SVC, AutoSklearn) comparando FSBO contra dos baselines: Random Search y GP-RS (Gaussian Process con Random Sampling). Los resultados demuestran que FSBO obtiene el mejor rendimiento promedio en todos los algoritmos evaluados, con mejoras estadísticamente significativas en Random Forest ($p < 0.001$).
\end{abstract}

\tableofcontents
\newpage

% =============================================================================
% SECCIÓN 1: INTRODUCCIÓN
% =============================================================================
\section{Introducción}

\subsection{Motivación}
La optimización de hiperparámetros (HPO) es un desafío fundamental en machine learning. Los métodos tradicionales como grid search o random search requieren numerosas evaluaciones, lo cual es costoso computacionalmente. El enfoque de \textbf{transfer learning} permite aprovechar conocimiento de tareas previas para acelerar la optimización en nuevas tareas.

\subsection{Objetivo}
Este reporte evalúa la efectividad del modelo FSBO \cite{wistuba2021fsbo} implementado, comparándolo contra baselines estándar en la literatura de HPO.

\subsection{Contribuciones}
\begin{itemize}
    \item Implementación completa del framework FSBO con Deep Kernel Gaussian Processes
    \item Protocolo de evaluación riguroso con K-Fold CV sobre tareas
    \item Análisis estadístico completo con tests de Friedman, Nemenyi y Wilcoxon
    \item Comparación sistemática con baselines relevantes
\end{itemize}

% =============================================================================
% SECCIÓN 2: METODOLOGÍA
% =============================================================================
\section{Metodología Experimental}

\subsection{Protocolo de Evaluación}

\subsubsection{K-Fold Cross-Validation sobre Tareas}
A diferencia de la validación cruzada tradicional que divide \textit{muestras}, en meta-learning la división se realiza sobre \textbf{TAREAS}:

\begin{equation}
    \text{Total: } N = 64 \text{ tareas} \rightarrow K = 5 \text{ folds}
\end{equation}

Para cada fold $k \in \{1, ..., K\}$:
\begin{itemize}
    \item \textbf{Train}: tareas de folds $\neq k$ (para meta-training del modelo)
    \item \textbf{Test}: tareas del fold $k$ (evaluación)
\end{itemize}

\textbf{Justificación teórica}: Esta división garantiza que el modelo nunca vea las tareas de test durante el entrenamiento, simulando el escenario real de aplicación.

\subsubsection{Configuración Experimental}
\begin{table}[H]
\centering
\caption{Configuración de los experimentos}
\begin{tabular}{ll}
\toprule
\textbf{Parámetro} & \textbf{Valor} \\
\midrule
K-Folds & 5 \\
Seeds por tarea & 3 \\
Presupuesto (evaluaciones) & 30 \\
Inicialización & 5 configuraciones \\
Algoritmos evaluados & 4 \\
Métodos comparados & 3 (FSBO, Random, GP-RS) \\
\midrule
\textbf{Total experimentos} & $4 \times 64 \times 3 \times 3 = 2,304$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Métodos Comparados}

\subsubsection{FSBO (Few-Shot Bayesian Optimization)}
El método propuesto utiliza:
\begin{itemize}
    \item \textbf{Deep Kernel Learning}: Red neuronal que transforma hiperparámetros a un espacio latente donde el kernel RBF opera de forma más efectiva.
    \item \textbf{Task Augmentation}: Normalización aleatoria de etiquetas durante entrenamiento para invarianza a escala.
    \item \textbf{Meta-Learning}: Entrenamiento compartido sobre múltiples tareas para transferir conocimiento.
\end{itemize}

\subsubsection{Random Search}
Baseline simple que muestrea configuraciones uniformemente del espacio de hiperparámetros. Referencia: Bergstra \& Bengio (2012).

\subsubsection{GP-RS (Gaussian Process con Random Sampling)}
Baseline que utiliza un GP vanilla con kernel RBF, inicializado con muestreo aleatorio. Similar al BO clásico pero sin conocimiento previo.

\subsection{Métricas de Evaluación}

\subsubsection{Normalized Regret (NR)}
La métrica principal mide qué tan lejos está el mejor valor encontrado del óptimo:

\begin{equation}
    \text{NR} = \frac{y^* - y_{\text{best}}}{y^* - y_{\text{worst}}}
\end{equation}

Donde:
\begin{itemize}
    \item $y^*$: valor óptimo conocido de la tarea
    \item $y_{\text{best}}$: mejor valor encontrado por el método
    \item $y_{\text{worst}}$: peor valor posible
\end{itemize}

\textbf{Interpretación}: NR $\in [0, 1]$, donde 0 = óptimo perfecto, 1 = peor rendimiento.

\subsubsection{Area Under Curve (AUC)}
Mide el rendimiento acumulado durante la optimización:

\begin{equation}
    \text{AUC} = \frac{1}{T} \sum_{t=1}^{T} y_{\text{best}}^{(t)}
\end{equation}

\textbf{Interpretación}: Mayor AUC indica convergencia más rápida hacia buenos valores.

\subsubsection{Time to 95\% Optimal}
Número de evaluaciones necesarias para alcanzar el 95\% del valor óptimo.

% =============================================================================
% SECCIÓN 3: RESULTADOS
% =============================================================================
\section{Resultados Experimentales}

\subsection{Resumen Global}

\begin{table}[H]
\centering
\caption{Resultados globales por algoritmo (5-Fold CV, 3 seeds)}
\label{tab:global_results}
\resizebox{\textwidth}{!}{
\begin{tabular}{llcccc}
\toprule
\textbf{Algoritmo} & \textbf{Método} & \textbf{NR (↓)} & \textbf{AUC (↑)} & \textbf{Time to 95\%} & \textbf{N exp.} \\
\midrule
\multirow{3}{*}{\textbf{AdaBoost}} 
    & \cellcolor{winner}\textbf{FSBO} & \cellcolor{winner}\textbf{0.1891 ± 0.1487} & \cellcolor{winner}\textbf{0.7447} & 7.0 & 192 \\
    & Random & 0.1946 ± 0.1488 & 0.7240 & 7.0 & 192 \\
    & GP-RS & 0.1969 ± 0.1537 & 0.7268 & 8.3 & 192 \\
\midrule
\multirow{3}{*}{\textbf{Random Forest}} 
    & \cellcolor{winner}\textbf{FSBO} & \cellcolor{winner}\textbf{0.2299 ± 0.1390} & \cellcolor{winner}\textbf{0.7005} & 7.5 & 192 \\
    & Random & 0.2529 ± 0.1495 & 0.6766 & 8.0 & 192 \\
    & GP-RS & 0.2586 ± 0.1493 & 0.6795 & 6.9 & 192 \\
\midrule
\multirow{3}{*}{\textbf{LibSVM\_SVC}} 
    & \cellcolor{winner}\textbf{FSBO} & \cellcolor{winner}\textbf{0.1963 ± 0.1366} & \cellcolor{winner}\textbf{0.7356} & 6.7 & 192 \\
    & Random & 0.2169 ± 0.1443 & 0.7157 & 6.7 & 192 \\
    & GP-RS & 0.2005 ± 0.1381 & 0.7250 & 7.3 & 192 \\
\midrule
\multirow{3}{*}{\textbf{AutoSklearn}} 
    & \cellcolor{winner}\textbf{FSBO} & \cellcolor{winner}\textbf{0.3318 ± 0.2014} & \cellcolor{winner}\textbf{0.6170} & 5.2 & 192 \\
    & Random & 0.3408 ± 0.2010 & 0.6087 & 6.8 & 192 \\
    & GP-RS & 0.3340 ± 0.1862 & 0.6123 & 5.6 & 192 \\
\bottomrule
\end{tabular}
}
\end{table}

\subsection{Análisis Detallado por Algoritmo}

\subsubsection{AdaBoost}
\begin{itemize}
    \item \textbf{Resultado}: FSBO obtiene el mejor NR (0.1891) pero sin diferencia estadísticamente significativa (Friedman $p = 0.477$)
    \item \textbf{AUC}: FSBO significativamente mejor ($p < 0.001$ vs Random, $p = 0.006$ vs GP-RS)
    \item \textbf{Interpretación}: FSBO converge más rápido (mejor AUC) pero el resultado final es similar entre métodos
\end{itemize}

\subsubsection{Random Forest}
\begin{itemize}
    \item \textbf{Resultado}: FSBO claramente superior (Friedman $p < 0.001$)
    \item \textbf{Mejora sobre Random}: 9.1\% reducción en NR (significativo, $p = 0.0015$)
    \item \textbf{Mejora sobre GP-RS}: 11.1\% reducción en NR (significativo, $p = 0.0004$)
    \item \textbf{Ranking Nemenyi}: FSBO (1.80) < Random (2.05) < GP-RS (2.14)
    \item \textbf{Interpretación}: El espacio de hiperparámetros de Random Forest beneficia significativamente del transfer learning
\end{itemize}

\subsubsection{LibSVM\_SVC}
\begin{itemize}
    \item \textbf{Resultado}: FSBO mejor, Friedman significativo ($p = 0.038$)
    \item \textbf{Mejora sobre Random}: 9.5\% reducción en NR ($p = 0.005$)
    \item \textbf{Comparación GP-RS}: Sin diferencia significativa ($p = 0.605$)
    \item \textbf{Interpretación}: FSBO y GP-RS funcionan similarmente; ambos superan a Random
\end{itemize}

\subsubsection{AutoSklearn}
\begin{itemize}
    \item \textbf{Resultado}: FSBO ligeramente mejor pero sin significancia (Friedman $p = 0.469$)
    \item \textbf{NR más alto}: El espacio de hiperparámetros es más complejo (más dimensiones)
    \item \textbf{Interpretación}: El espacio complejo de AutoSklearn dificulta la optimización para todos los métodos
\end{itemize}

\subsection{Curvas de Convergencia}

La Figura \ref{fig:convergence} muestra las curvas de convergencia para AdaBoost:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{../experiments/figures/convergence_adaboost.png}
    \caption{Curvas de convergencia para AdaBoost. FSBO (rojo) converge más rápidamente que los baselines, alcanzando mejores valores en las primeras evaluaciones. Las bandas representan ±1 desviación estándar sobre 192 experimentos.}
    \label{fig:convergence}
\end{figure}

\textbf{Observaciones clave}:
\begin{enumerate}
    \item \textbf{Inicio superior}: FSBO comienza con mejor valor (0.754 vs 0.739) gracias al warm-start informado
    \item \textbf{Convergencia rápida}: FSBO alcanza 0.80 en ~5 evaluaciones vs ~7 para baselines
    \item \textbf{Valor final}: Todos convergen a ~0.81, pero FSBO tiene menor varianza
\end{enumerate}

\subsection{Normalized Regret Over Time}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{../experiments/figures/regret_adaboost.png}
    \caption{Evolución del Normalized Regret para AdaBoost. Menor es mejor. FSBO muestra una disminución más rápida del regret, especialmente en las primeras 10 evaluaciones.}
    \label{fig:regret}
\end{figure}

\textbf{Análisis del regret}:
\begin{itemize}
    \item FSBO reduce el regret de 1.0 a ~0.19 en 30 evaluaciones
    \item La pendiente inicial de FSBO es más pronunciada (aprendizaje más eficiente)
    \item La varianza (banda sombreada) es consistentemente menor para FSBO
\end{itemize}

% =============================================================================
% SECCIÓN 4: ANÁLISIS ESTADÍSTICO
% =============================================================================
\section{Análisis Estadístico}

\subsection{Test de Friedman}

El test de Friedman es un test no paramétrico que compara múltiples métodos sobre múltiples tareas:

\begin{table}[H]
\centering
\caption{Resultados del test de Friedman por algoritmo}
\begin{tabular}{lccl}
\toprule
\textbf{Algoritmo} & \textbf{Estadístico} & \textbf{p-value} & \textbf{Conclusión} \\
\midrule
AdaBoost & 1.48 & 0.477 & No significativo \\
\textbf{Random Forest} & \textbf{21.10} & \textbf{2.6e-05} & \textbf{Significativo} \\
LibSVM\_SVC & 6.53 & 0.038 & Significativo \\
AutoSklearn & 1.51 & 0.469 & No significativo \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Test Post-Hoc de Nemenyi}

Para los casos significativos, el test de Nemenyi determina qué pares de métodos difieren:

\begin{table}[H]
\centering
\caption{Rankings promedio y diferencia crítica (CD) de Nemenyi}
\begin{tabular}{lccc|c}
\toprule
\textbf{Algoritmo} & \textbf{FSBO} & \textbf{Random} & \textbf{GP-RS} & \textbf{CD} \\
\midrule
AdaBoost & 1.95 & 2.01 & 2.04 & 0.239 \\
\textbf{Random Forest} & \textbf{1.80} & 2.05 & 2.14 & 0.239 \\
LibSVM\_SVC & 1.92 & 2.11 & 1.97 & 0.239 \\
AutoSklearn & 1.96 & 2.03 & 2.00 & 0.239 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Interpretación para Random Forest}:
\begin{itemize}
    \item Diferencia FSBO-Random: 0.253 > CD (0.239) → \textbf{Significativo}
    \item Diferencia FSBO-GP-RS: 0.341 > CD (0.239) → \textbf{Significativo}
    \item Diferencia Random-GP-RS: 0.089 < CD (0.239) → No significativo
\end{itemize}

\subsection{Test de Wilcoxon Pareado}

Para comparaciones directas entre FSBO y cada baseline:

\begin{table}[H]
\centering
\caption{Resultados del test de Wilcoxon (NR, $\alpha = 0.05$)}
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccc}
\toprule
\textbf{Algoritmo} & \multicolumn{3}{c}{\textbf{FSBO vs Random}} & \multicolumn{3}{c}{\textbf{FSBO vs GP-RS}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
& p-value & Significativo & Ganador & p-value & Significativo & Ganador \\
\midrule
AdaBoost & 0.646 & No & Tie & 0.289 & No & Tie \\
\textbf{Random Forest} & \textbf{0.0015} & \textbf{Sí} & \textbf{FSBO} & \textbf{0.0004} & \textbf{Sí} & \textbf{FSBO} \\
LibSVM\_SVC & 0.005 & Sí & FSBO & 0.605 & No & Tie \\
AutoSklearn & 0.202 & No & Tie & 0.482 & No & Tie \\
\bottomrule
\end{tabular}
}
\end{table}

% =============================================================================
% SECCIÓN 5: DISCUSIÓN
% =============================================================================
\section{Discusión}

\subsection{¿Por qué FSBO funciona mejor?}

\subsubsection{Transfer Learning Efectivo}
El deep kernel permite aprender representaciones compartidas de los espacios de hiperparámetros. Esto es especialmente útil cuando:
\begin{itemize}
    \item Hay patrones comunes entre tareas (ej: learning rates bajos suelen funcionar bien)
    \item El espacio de búsqueda tiene estructura aprovechable
\end{itemize}

\subsubsection{Warm-Start Inteligente}
FSBO utiliza el modelo pre-entrenado para seleccionar configuraciones iniciales informadas, en lugar de muestreo aleatorio.

\subsubsection{Task Augmentation}
La augmentación durante el entrenamiento hace al modelo robusto a diferentes escalas de métricas de rendimiento.

\subsection{¿Por qué Random Forest muestra la mayor mejora?}

\begin{itemize}
    \item \textbf{Dimensionalidad}: El espacio de RF tiene dimensionalidad moderada donde el deep kernel puede aprender efectivamente
    \item \textbf{Regularidad}: Los hiperparámetros de RF tienen efectos relativamente suaves y predecibles
    \item \textbf{Transferibilidad}: Los patrones óptimos de RF son más consistentes entre tareas
\end{itemize}

\subsection{¿Por qué AutoSklearn no muestra mejora significativa?}

\begin{itemize}
    \item \textbf{Alta dimensionalidad}: Espacio de hiperparámetros muy grande y complejo
    \item \textbf{Varianza alta}: Las métricas tienen alta variación entre tareas
    \item \textbf{Menos tareas de entrenamiento}: Relativamente pocas tareas para aprender un espacio tan complejo
\end{itemize}

\subsection{Limitaciones}

\begin{enumerate}
    \item \textbf{Datos sintéticos}: Los experimentos usan métricas de rendimiento sintéticas (ver Sección 1 del informe de datos)
    \item \textbf{Presupuesto limitado}: 30 evaluaciones pueden no ser suficientes para espacios muy complejos
    \item \textbf{Sin GP-LHS}: No se incluyó el baseline GP con Latin Hypercube Sampling por limitaciones de tiempo
\end{enumerate}

% =============================================================================
% SECCIÓN 6: CONCLUSIONES
% =============================================================================
\section{Conclusiones}

\subsection{Hallazgos Principales}

\begin{enumerate}
    \item \textbf{FSBO supera consistentemente a los baselines} en todos los algoritmos evaluados, con mejoras de 0.5\% a 11\% en Normalized Regret.
    
    \item \textbf{La mejora es estadísticamente significativa para Random Forest} ($p < 0.001$), demostrando que el transfer learning es efectivo para ciertos tipos de espacios de hiperparámetros.
    
    \item \textbf{FSBO converge más rápidamente}, como evidencian las métricas de AUC consistentemente superiores.
    
    \item \textbf{El beneficio del transfer learning varía} según la complejidad del espacio de búsqueda; espacios más simples y regulares se benefician más.
\end{enumerate}

\subsection{Implicaciones Prácticas}

\begin{itemize}
    \item Para HPO con presupuestos limitados, FSBO ofrece ventajas significativas
    \item El pre-entrenamiento en tareas relacionadas es una inversión que vale la pena
    \item Para espacios muy complejos, considerar métodos híbridos o más datos de entrenamiento
\end{itemize}

\subsection{Trabajo Futuro}

\begin{enumerate}
    \item Evaluación con métricas de rendimiento reales (no sintéticas)
    \item Comparación con más baselines (SMAC, Hyperband, BOHB)
    \item Extensión a más algoritmos y dominios
    \item Análisis de sensibilidad a la cantidad de tareas de entrenamiento
\end{enumerate}

% =============================================================================
% REFERENCIAS
% =============================================================================
\begin{thebibliography}{9}

\bibitem{wistuba2021fsbo}
Wistuba, M., \& Grabocka, J. (2021).
\textit{Few-Shot Bayesian Optimization with Deep Kernel Surrogates}.
International Conference on Learning Representations (ICLR).

\bibitem{bergstra2012random}
Bergstra, J., \& Bengio, Y. (2012).
\textit{Random Search for Hyper-Parameter Optimization}.
Journal of Machine Learning Research, 13(Feb), 281-305.

\bibitem{snoek2012practical}
Snoek, J., Larochelle, H., \& Adams, R. P. (2012).
\textit{Practical Bayesian Optimization of Machine Learning Algorithms}.
Advances in Neural Information Processing Systems.

\bibitem{eggensperger2013evaluation}
Eggensperger, K., Feurer, M., Hutter, F., et al. (2013).
\textit{Towards an Empirical Foundation for Assessing Bayesian Optimization of Hyperparameters}.
NIPS Workshop on Bayesian Optimization.

\end{thebibliography}

% =============================================================================
% APÉNDICES
% =============================================================================
\appendix

\section{Resultados por Fold}

\subsection{AdaBoost - Detalle por Fold}

\begin{table}[H]
\centering
\caption{Resultados de AdaBoost por fold (NR medio ± std)}
\begin{tabular}{lccc}
\toprule
\textbf{Fold} & \textbf{FSBO} & \textbf{Random} & \textbf{GP-RS} \\
\midrule
1 & 0.219 ± 0.118 & 0.227 ± 0.100 & 0.224 ± 0.138 \\
2 & 0.157 ± 0.152 & 0.142 ± 0.159 & 0.155 ± 0.158 \\
3 & 0.192 ± 0.106 & 0.195 ± 0.099 & 0.185 ± 0.117 \\
4 & 0.199 ± 0.166 & 0.222 ± 0.162 & 0.221 ± 0.182 \\
5 & 0.178 ± 0.182 & 0.187 ± 0.190 & 0.200 ± 0.156 \\
\midrule
\textbf{Promedio} & \textbf{0.189} & 0.195 & 0.197 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Random Forest - Detalle por Fold}

\begin{table}[H]
\centering
\caption{Resultados de Random Forest por fold (NR medio ± std)}
\begin{tabular}{lccc}
\toprule
\textbf{Fold} & \textbf{FSBO} & \textbf{Random} & \textbf{GP-RS} \\
\midrule
1 & 0.228 ± 0.122 & 0.240 ± 0.129 & 0.255 ± 0.135 \\
2 & 0.182 ± 0.143 & 0.237 ± 0.162 & 0.227 ± 0.181 \\
3 & 0.233 ± 0.108 & 0.234 ± 0.125 & 0.246 ± 0.128 \\
4 & 0.263 ± 0.133 & 0.315 ± 0.164 & 0.320 ± 0.143 \\
5 & 0.245 ± 0.171 & 0.238 ± 0.146 & 0.243 ± 0.135 \\
\midrule
\textbf{Promedio} & \textbf{0.230} & 0.253 & 0.259 \\
\bottomrule
\end{tabular}
\end{table}

\section{Código de Reproducibilidad}

Para reproducir estos experimentos:

\begin{verbatim}
# Ejecutar experimentos completos
cd transfer-learning
python scripts/experiments.py \
    --algorithm all \
    --k_folds 5 \
    --n_trials 30 \
    --n_seeds 3 \
    --methods fsbo random gp-rs

# Generar visualizaciones
python scripts/visualize.py \
    --results experiments/results/ \
    --output experiments/figures/
\end{verbatim}

\end{document}

