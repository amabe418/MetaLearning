"""
Synthetic Hyperparameter Configuration Generator
Basado en t√©cnicas de: 
- BOPrO (Bayesian Optimization with Priors): Gaussian noise
- SMAC3 (Sequential Model-Based Algorithm Configuration): Surrogate models
- van Rijn & Hutter (2017): Hyperparameter sampling

Genera configuraciones sint√©ticas a partir de ejecuciones reales.
"""

import pandas as pd
import numpy as np
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# CONFIGURACI√ìN
# ============================================================================

# Hiperpar√°metros num√©ricos a perturbar
NUMERIC_PARAMS = [
    'learning_rate',
    'batch_size',
    'weight_decay',
    'momentum',
    'dropout_rate',
    'alpha',
    'label_smoothing',
    'grad_clip'
]

# L√≠mites para cada hiperpar√°metro (min, max)
PARAM_BOUNDS = {
    'learning_rate': (0.0001, 0.1),
    'batch_size': (16, 128),
    'weight_decay': (0.0, 0.01),
    'momentum': (0.0, 0.99),
    'dropout_rate': (0.0, 0.5),
    'alpha': (0.5, 1.0),
    'label_smoothing': (0.0, 0.3),
    'grad_clip': (0.0, 5.0),
}

# Tipo de distribuci√≥n para cada par√°metro
PARAM_TYPES = {
    'learning_rate': 'log',       # Escala logar√≠tmica
    'batch_size': 'discrete',     # Entero (m√∫ltiplo de 8)
    'weight_decay':  'log',        # Escala logar√≠tmica
    'momentum': 'uniform',        # Escala uniforme
    'dropout_rate': 'uniform',
    'alpha': 'uniform',
    'label_smoothing':  'uniform',
    'grad_clip': 'uniform',
}

# M√©tricas a interpolar
TARGET_METRICS = ['test_accuracy', 'train_accuracy', 'test_loss']

# ============================================================================
# FUNCI√ìN 1: GENERACI√ìN DE RUIDO GAUSSIANO (Basada en BOPrO)
# ============================================================================

def generate_gaussian_noise_configs(df_real, num_synthetic_per_config=5, noise_std=0.15, seed=42):
    """
    Genera configuraciones sint√©ticas a√±adiendo ruido gaussiano a las reales.
    
    Basado en: 
    - BOPrO (Balandat et al., 2020): https://arxiv.org/abs/2002.10389
    - SMAC3 (Hutter et al., 2011): Sampling strategies
    
    Args:
        df_real: DataFrame con configuraciones reales
        num_synthetic_per_config: N√∫mero de variaciones por cada config real
        noise_std: Desviaci√≥n est√°ndar del ruido (como % del valor)
        seed: Semilla para reproducibilidad
    
    Returns:
        DataFrame con configuraciones sint√©ticas (sin m√©tricas interpoladas)
    """
    np.random.seed(seed)
    
    print(f"\n{'='*70}")
    print(f"üé≤ GENERANDO CONFIGURACIONES SINT√âTICAS CON RUIDO GAUSSIANO")
    print(f"{'='*70}")
    print(f"Configuraciones reales: {len(df_real)}")
    print(f"Variaciones por config:  {num_synthetic_per_config}")
    print(f"Nivel de ruido (std): {noise_std}")
    
    synthetic_rows = []
    
    for idx, real_row in df_real.iterrows():
        for i in range(num_synthetic_per_config):
            synthetic_row = real_row.copy()
            
            # Perturbar cada hiperpar√°metro num√©rico
            for param in NUMERIC_PARAMS:
                if param not in real_row or pd.isna(real_row[param]):
                    continue
                
                original_value = float(real_row[param])
                param_type = PARAM_TYPES[param]
                min_val, max_val = PARAM_BOUNDS[param]
                
                # Generar ruido seg√∫n el tipo de par√°metro
                if param_type == 'log':
                    # Ruido en escala logar√≠tmica (para LR, weight_decay)
                    if original_value > 0:
                        log_val = np.log10(original_value + 1e-10)
                        noise = np.random.normal(0, noise_std)
                        new_log_val = log_val + noise
                        new_value = 10 ** new_log_val
                    else:
                        new_value = original_value
                    
                elif param_type == 'discrete':
                    # Ruido para valores discretos (batch_size)
                    noise = np.random.normal(0, 16)  # std de 16
                    new_value = int(original_value + noise)
                    # Redondear a m√∫ltiplo de 8 (eficiencia GPU)
                    new_value = max(8, (new_value // 8) * 8)
                    
                else:  # 'uniform'
                    # Ruido proporcional al valor
                    if original_value > 0:
                        noise = np.random. normal(0, noise_std * original_value)
                        new_value = original_value + noise
                    else:
                        # Si es 0, aplicar ruido peque√±o absoluto
                        noise = np.random.normal(0, noise_std * 0.1)
                        new_value = max(0, noise)
                
                # Aplicar l√≠mites
                new_value = np.clip(new_value, min_val, max_val)
                
                # Actualizar
                synthetic_row[param] = new_value
            
            # NO modificar las m√©tricas todav√≠a (se interpolar√°n despu√©s)
            for metric in TARGET_METRICS: 
                if metric in synthetic_row:
                    synthetic_row[metric] = np.nan
            
            synthetic_rows. append(synthetic_row)
    
    df_synthetic = pd.DataFrame(synthetic_rows)
    
    print(f"‚úì Generadas {len(df_synthetic)} configuraciones sint√©ticas")
    print(f"{'='*70}\n")
    
    return df_synthetic

# ============================================================================
# FUNCI√ìN 2: INTERPOLACI√ìN CON SURROGATE MODEL (Basada en SMAC3)
# ============================================================================

def interpolate_metrics_with_surrogate(df_synthetic, df_real, method='knn', k=5):
    """
    Interpola m√©tricas de configs sint√©ticas usando surrogate models.
    
    Basado en:
    - SMAC3: Random Forest surrogate
    - Optuna: K-NN + Tree-structured Parzen Estimator
    - van Rijn & Hutter (2017): Hyperparameter importance
    
    Args:
        df_synthetic: DataFrame con configs sint√©ticas (sin m√©tricas)
        df_real: DataFrame con configs reales (con m√©tricas)
        method: 'knn', 'random_forest', 'ensemble'
        k: N√∫mero de vecinos (para KNN)
    
    Returns:
        DataFrame sint√©tico con m√©tricas interpoladas
    """
    print(f"\n{'='*70}")
    print(f"üîÑ INTERPOLANDO M√âTRICAS CON SURROGATE MODEL")
    print(f"{'='*70}")
    print(f"M√©todo: {method. upper()}")
    print(f"Configuraciones reales: {len(df_real)}")
    print(f"Configuraciones sint√©ticas: {len(df_synthetic)}")
    
    # Preparar datos
    X_real = df_real[NUMERIC_PARAMS].fillna(0).values
    X_synthetic = df_synthetic[NUMERIC_PARAMS].fillna(0).values
    
    # Normalizar features
    scaler = StandardScaler()
    X_real_scaled = scaler.fit_transform(X_real)
    X_synthetic_scaled = scaler.transform(X_synthetic)
    
    # Interpolar cada m√©trica
    for metric in TARGET_METRICS: 
        if metric not in df_real.columns:
            print(f"  ‚ö†Ô∏è  M√©trica '{metric}' no encontrada, saltando...")
            continue
        
        y_real = df_real[metric].values
        
        # Elegir surrogate model
        if method == 'knn':
            # K-NN con ponderaci√≥n por distancia
            model = KNeighborsRegressor(n_neighbors=k, weights='distance')
        
        elif method == 'random_forest':
            # Random Forest (como SMAC3)
            model = RandomForestRegressor(
                n_estimators=100,
                max_depth=10,
                min_samples_split=5,
                random_state=42,
                n_jobs=-1
            )
        
        elif method == 'ensemble':
            # Ensemble de KNN + RF (m√°s robusto)
            from sklearn.ensemble import VotingRegressor
            knn = KNeighborsRegressor(n_neighbors=k, weights='distance')
            rf = RandomForestRegressor(n_estimators=50, max_depth=8, random_state=42)
            model = VotingRegressor([('knn', knn), ('rf', rf)])
        
        else:
            raise ValueError(f"M√©todo '{method}' no soportado")
        
        # Entrenar modelo
        model.fit(X_real_scaled, y_real)
        
        # Predecir para configs sint√©ticas
        y_synthetic = model.predict(X_synthetic_scaled)
        
        # Calcular confianza (validaci√≥n cruzada en datos reales)
        cv_scores = cross_val_score(model, X_real_scaled, y_real, cv=3, scoring='r2')
        confidence = cv_scores.mean()
        
        # Actualizar DataFrame
        df_synthetic[metric] = y_synthetic
        
        print(f"  ‚úì {metric: 20s} | R¬≤ CV: {confidence:.4f} | "
              f"Range: [{y_synthetic.min():.4f}, {y_synthetic.max():.4f}]")
    
    print(f"{'='*70}\n")
    
    return df_synthetic

# ============================================================================
# FUNCI√ìN 3: VALIDACI√ìN DE CONFIGS SINT√âTICAS
# ============================================================================

def validate_synthetic_configs(df_synthetic, df_real):
    """
    Valida que las configs sint√©ticas sean realistas comparando distribuciones.
    """
    print(f"\n{'='*70}")
    print(f"üìä VALIDACI√ìN DE CONFIGURACIONES SINT√âTICAS")
    print(f"{'='*70}")
    
    for param in NUMERIC_PARAMS: 
        real_mean = df_real[param].mean()
        real_std = df_real[param].std()
        
        synth_mean = df_synthetic[param].mean()
        synth_std = df_synthetic[param].std()
        
        # Verificar que la distribuci√≥n sint√©tica sea similar a la real
        mean_diff = abs(synth_mean - real_mean) / real_mean if real_mean != 0 else 0
        std_diff = abs(synth_std - real_std) / real_std if real_std != 0 else 0
        
        status = "‚úì" if mean_diff < 0.3 and std_diff < 0.5 else "‚ö†Ô∏è"
        
        print(f"  {status} {param: 20s} | "
              f"Mean:  {real_mean:.4f}‚Üí{synth_mean:.4f} ({mean_diff*100:+.1f}%) | "
              f"Std: {real_std:.4f}‚Üí{synth_std:. 4f} ({std_diff*100:+.1f}%)")
    
    print(f"{'='*70}\n")

# ============================================================================
# FUNCI√ìN 4: AN√ÅLISIS COMPARATIVO
# ============================================================================

def comparative_analysis(df_real, df_synthetic, df_combined):
    """
    Genera estad√≠sticas comparativas entre configs reales y sint√©ticas.
    """
    print(f"\n{'='*70}")
    print(f"üìà AN√ÅLISIS COMPARATIVO")
    print(f"{'='*70}")
    
    print(f"\nüìå RESUMEN:")
    print(f"  Configuraciones reales:      {len(df_real):,}")
    print(f"  Configuraciones sint√©ticas:  {len(df_synthetic):,}")
    print(f"  Total final:                {len(df_combined):,}")
    print(f"  Factor de aumento:          {len(df_combined)/len(df_real):.1f}x")
    
    print(f"\nüìå DISTRIBUCI√ìN POR ARQUITECTURA:")
    for arch in df_combined['architecture'].unique():
        real_count = len(df_real[df_real['architecture'] == arch])
        synth_count = len(df_synthetic[df_synthetic['architecture'] == arch])
        print(f"  {arch:20s} | Real: {real_count:4d} | Sint√©tico: {synth_count:4d}")
    
    print(f"\nüìå DISTRIBUCI√ìN POR DATASET (primeros 10):")
    for task in df_combined['task_id'].unique()[:10]:
        real_count = len(df_real[df_real['task_id'] == task])
        synth_count = len(df_synthetic[df_synthetic['task_id'] == task])
        print(f"  {task:20s} | Real: {real_count:4d} | Sint√©tico: {synth_count:4d}")
    
    print(f"\nüìå ESTAD√çSTICAS DE M√âTRICAS:")
    for metric in TARGET_METRICS:
        if metric in df_combined.columns:
            print(f"\n  {metric}:")
            print(f"    Real      - Mean: {df_real[metric]. mean():.4f} | "
                  f"Std:  {df_real[metric].std():.4f} | "
                  f"Range: [{df_real[metric].min():.4f}, {df_real[metric].max():.4f}]")
            print(f"    Sint√©tico - Mean:  {df_synthetic[metric].mean():.4f} | "
                  f"Std: {df_synthetic[metric].std():.4f} | "
                  f"Range: [{df_synthetic[metric].min():.4f}, {df_synthetic[metric].max():.4f}]")
    
    print(f"\n{'='*70}\n")

# ============================================================================
# FUNCI√ìN PRINCIPAL
# ============================================================================

def augment_hyperparameter_data(
    input_csv='target_representations.csv',
    output_csv='target_representations_augmented.csv',
    num_synthetic_per_config=5,
    noise_std=0.15,
    interpolation_method='knn',
    k_neighbors=5,
    seed=42
):
    """
    Pipeline completo para augmentar datos de hiperpar√°metros.
    
    Args:
        input_csv: CSV con configuraciones reales
        output_csv: CSV de salida con configs reales + sint√©ticas
        num_synthetic_per_config:  Cu√°ntas variaciones generar por config real
        noise_std: Nivel de ruido gaussiano (0. 15 = 15%)
        interpolation_method: 'knn', 'random_forest', 'ensemble'
        k_neighbors:  N√∫mero de vecinos para KNN
        seed: Semilla para reproducibilidad
    
    Returns:
        df_combined: DataFrame con todos los datos
    """
    print(f"\n{'='*70}")
    print(f"üöÄ PIPELINE DE AUGMENTACI√ìN DE HIPERPAR√ÅMETROS")
    print(f"{'='*70}")
    print(f"Input:   {input_csv}")
    print(f"Output: {output_csv}")
    print(f"M√©todo: {interpolation_method. upper()}")
    print(f"{'='*70}\n")
    
    # 1. Cargar datos reales
    print("üìÇ Cargando configuraciones reales...")
    df_real = pd.read_csv(input_csv)
    print(f"‚úì Cargadas {len(df_real)} configuraciones reales")
    print(f"‚úì Datasets √∫nicos: {df_real['task_id'].nunique()}")
    print(f"‚úì Arquitecturas:  {df_real['architecture'].unique().tolist()}\n")
    
    # 2. Generar configs sint√©ticas con ruido gaussiano
    df_synthetic = generate_gaussian_noise_configs(
        df_real,
        num_synthetic_per_config=num_synthetic_per_config,
        noise_std=noise_std,
        seed=seed
    )
    
    # 3. Interpolar m√©tricas con surrogate model
    df_synthetic = interpolate_metrics_with_surrogate(
        df_synthetic,
        df_real,
        method=interpolation_method,
        k=k_neighbors
    )
    
    # 4. Validar configs sint√©ticas
    validate_synthetic_configs(df_synthetic, df_real)
    
    # 5. Combinar real + sint√©tico
    df_combined = pd. concat([df_real, df_synthetic], ignore_index=True)
    
    # 6. A√±adir columna de identificaci√≥n
    df_combined['is_synthetic'] = ['Real'] * len(df_real) + ['Synthetic'] * len(df_synthetic)
    
    # 7. Guardar
    df_combined.to_csv(output_csv, index=False)
    print(f"üíæ Archivo guardado: {output_csv}")
    
    # 8. An√°lisis comparativo
    comparative_analysis(df_real, df_synthetic, df_combined)
    
    # 9. Resumen final
    print(f"\n{'='*70}")
    print(f"‚úÖ PIPELINE COMPLETADO")
    print(f"{'='*70}")
    print(f"Total configuraciones:  {len(df_combined):,}")
    print(f"  - Reales:      {len(df_real):,} ({len(df_real)/len(df_combined)*100:.1f}%)")
    print(f"  - Sint√©ticas: {len(df_synthetic):,} ({len(df_synthetic)/len(df_combined)*100:.1f}%)")
    print(f"Factor de aumento: {len(df_combined)/len(df_real):.1f}x")
    print(f"{'='*70}\n")
    
    return df_combined

# ============================================================================
# SCRIPT PRINCIPAL
# ============================================================================

if __name__ == '__main__':
    # Configuraci√≥n
    INPUT_CSV = 'target_representations.csv'
    OUTPUT_CSV = 'target_representations_augmented.csv'
    
    # Par√°metros de augmentaci√≥n
    NUM_SYNTHETIC_PER_CONFIG = 5   # 180 √ó 5 = 900 sint√©ticas ‚Üí Total: 1,080
    NOISE_STD = 0.15               # 15% de desviaci√≥n est√°ndar
    INTERPOLATION_METHOD = 'knn'   # Opciones: 'knn', 'random_forest', 'ensemble'
    K_NEIGHBORS = 5                # Para KNN
    
    # Ejecutar pipeline
    df_augmented = augment_hyperparameter_data(
        input_csv=INPUT_CSV,
        output_csv=OUTPUT_CSV,
        num_synthetic_per_config=NUM_SYNTHETIC_PER_CONFIG,
        noise_std=NOISE_STD,
        interpolation_method=INTERPOLATION_METHOD,
        k_neighbors=K_NEIGHBORS,
        seed=42
    )
    
    # Mostrar preview
    print("\nüìä PREVIEW DEL DATASET AUGMENTADO:")
    print(df_augmented.head(15).to_string())
    
    print("\nüìà ESTAD√çSTICAS POR TIPO:")
    print(df_augmented.groupby('is_synthetic')['test_accuracy'].describe())