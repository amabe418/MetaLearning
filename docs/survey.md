# Survey ‚Äì Meta-Learning

# *Paper 1: Meta-Learning: A Survey (Joaquin Vanschoren et al., 2017)
**Link:** [Meta-Learning: A Survey](state_of_the_art/metaLearning.pdf)
**Idea principal:** explica ideas iniciales y referencias de varios autores de las diferentes formas de hacer metalearning

---

# Paper 2: Experiment databases (Vanschoren ¬∑ Blockeel ¬∑ Pfahringer ¬∑ Holmes 2012)
**Link:** [Experiment dataset](state_of_the_art/Experiment_databases.pdf)
**Idea principal:** aprendizaje de par√°metros iniciales que permiten adaptaci√≥n r√°pida.  
**Contribuciones clave:**
- Meta-learning basado en gradientes.
- Uso de inner/outer loops.
- SOTA en few-shot classification.

**Limitaciones:**
- Costoso computacionalmente.
- Inner loop inestable si el learning rate no es adecuado.

---

# Paper 3: Alpha D3M: Machine Learning Pipeline Synthsis

## üìå Contexto en el estado del arte

AlphaD3M se sit√∫a en la intersecci√≥n de:

* **Meta-learning**
* **AutoML**
* **S√≠ntesis autom√°tica de pipelines**
* **Reinforcement Learning profundo (AlphaZero-like)**

Pertenece a la l√≠nea de investigaci√≥n impulsada por **DARPA D3M**, cuyo objetivo es:

> Resolver *cualquier tarea ML* sobre *cualquier dataset*, sintetizando autom√°ticamente pipelines completos, **explicables y eficientes**.

---

## üéØ ¬øDe qu√© trata el paper?

El paper introduce **AlphaD3M**, un sistema AutoML que:

* Modela la **construcci√≥n de pipelines ML como un juego de un solo jugador**
* Usa:

  * **Redes neuronales recurrentes (LSTM)**
  * **Monte Carlo Tree Search (MCTS)**
  * **Auto-juego (self-play)** al estilo **AlphaZero**
* Aprende a **editar pipelines** mediante:

  * inserci√≥n
  * eliminaci√≥n
  * reemplazo de componentes

üëâ El resultado es un sistema:

* competitivo con AutoSklearn, TPOT y Autostacker
* **mucho m√°s r√°pido**
* **explicable por dise√±o**

---

## üß© Idea central (contribuci√≥n conceptual)

### üîë Reformulaci√≥n clave

> **La s√≠ntesis de pipelines es un problema de b√∫squeda secuencial**, no solo de optimizaci√≥n de hiperpar√°metros.

Se modela como:

| Concepto   | AlphaZero    | AlphaD3M                            |
| ---------- | ------------ | ----------------------------------- |
| Juego      | Ajedrez / Go | AutoML                              |
| Estado     | Tablero      | (Dataset + tarea + pipeline actual) |
| Acci√≥n     | Movimiento   | Editar pipeline                     |
| Recompensa | Ganar        | Performance del pipeline            |

---

## üß† ¬øQu√© parte del *meta-learning* explica?

AlphaD3M **NO se centra** en:

* selecci√≥n manual de meta-features
* ranking cl√°sico de algoritmos
* kNN meta-learning

üëâ Se centra en **meta-learning impl√≠cito**, aprendido por la red.

### Meta-learning en AlphaD3M =

> aprender **patrones recurrentes de pipelines efectivos** a trav√©s de m√∫ltiples datasets y tareas.

### Representaci√≥n del estado (meta-learning)

El estado incluye:

1. **Meta-data del dataset**
   (no detallado exhaustivamente en el paper)
2. **Definici√≥n de la tarea**
3. **Pipeline completo actual**
4. **Historial impl√≠cito de decisiones**

‚ö†Ô∏è Importante:

* **NO lista expl√≠citamente las meta-features**
* El *aprendizaje* ocurre dentro de la red neuronal

---

## üß™ ¬øQu√© datos y benchmarks usa?

* **313 datasets tabulares**

  * **296 de OpenML**
* Tareas:

  * Clasificaci√≥n binaria
  * Clasificaci√≥n multiclase
  * Regresi√≥n
* NO se listan expl√≠citamente:

  * `dataset_id`
  * `task_id`
  * suites OpenML

üëâ El paper **NO est√° orientado a reproducibilidad fina**, sino a demostrar el enfoque.

---

## ‚öôÔ∏è ¬øQu√© algoritmos/pipelines usa?

* Pipelines compuestos por:

  * Preprocesamiento
  * Feature extraction
  * Feature selection
  * Estimadores
  * Post-procesamiento
* Baseline:

  * SGD (clasificaci√≥n y regresi√≥n)
* Los algoritmos concretos **no se enumeran formalmente**
* Se trabaja con **primitives** (concepto D3M), no con ‚Äúalgoritmos aislados‚Äù

---

## üß† ¬øC√≥mo funciona el sistema (pipeline conceptual)?

```text
Dataset + Task
      ‚Üì
Estado inicial (pipeline base)
      ‚Üì
Red neuronal (LSTM)
  ‚Üí predice:
    - probabilidad de acciones
    - performance estimada
      ‚Üì
MCTS
  ‚Üí explora ediciones de pipeline
      ‚Üì
Evaluaci√≥n real del pipeline
      ‚Üì
Reward (performance)
      ‚Üì
Entrenamiento (self-play)
```

La red aprende **qu√© editar, cu√°ndo y por qu√©**.

---

## üìà Resultados principales

* AlphaD3M:

  * supera a pipelines base en ~75% de datasets
  * es **comparable en performance** con AutoSklearn, TPOT y Autostacker
  * es **~10√ó m√°s r√°pido**
* Tiempo:

  * horas ‚Üí minutos
* Ventaja clave:

  * **explicabilidad estructural** (ediciones del pipeline)

---

## üß† Aportes principales del paper

### ‚úîÔ∏è Conceptuales

* Primera formulaci√≥n **AlphaZero-like** para AutoML
* Pipeline synthesis como **juego secuencial**
* Meta-learning **end-to-end**, no basado en meta-features manuales

### ‚úîÔ∏è T√©cnicos

* Uso combinado de:

  * LSTM
  * MCTS
  * Self-play
* Ediciones de pipeline como acciones explicables

### ‚úîÔ∏è Pr√°cticos

* M√°s r√°pido que AutoML cl√°sico
* Escalable a espacios enormes de pipelines

---

## ‚ùå Qu√© NO explica (y por qu√©)

| Elemento                 | ¬øEst√°? | Raz√≥n                       |
| ------------------------ | ------ | --------------------------- |
| Meta-features expl√≠citas | ‚ùå      | Aprendidas impl√≠citamente   |
| IDs OpenML               | ‚ùå      | No es paper de benchmarking |
| Ranking de algoritmos    | ‚ùå      | Trabaja a nivel pipeline    |
| Reproducibilidad exacta  | ‚ùå      | Enfoque conceptual          |

Esto es **intencional**, no un fallo.

---

## üîó C√≥digo e implementaciones

### ‚úîÔ∏è Implementaci√≥n asociada (D3M / AlphaD3M)

* Proyecto D3M (DARPA):

  * [https://github.com/VIDA-NYU/d3m](https://github.com/VIDA-NYU/d3m)
* Componentes relacionados con AlphaD3M:

  * [https://github.com/VIDA-NYU/alphad3m](https://github.com/VIDA-NYU/alphad3m)

‚ö†Ô∏è Nota:

* El c√≥digo es **complejo**
* Usa primitives D3M
* No es tan ‚Äúplug-and-play‚Äù como AutoSklearn

---

## üß† Relaci√≥n con otros enfoques de meta-learning

| Enfoque               | Ejemplo         | AlphaD3M |
| --------------------- | --------------- | -------- |
| Meta-features + kNN   | Brazdil, Soares | ‚ùå        |
| Ranking de algoritmos | OpenML          | ‚ùå        |
| AutoML Bayesian       | AutoSklearn     | Parcial  |
| Evolutivo             | TPOT            | Parcial  |
| RL + search           | ‚ùå               | ‚úÖ        |

---

## üìù Resumen corto (para secci√≥n *Related Work*)

> *AlphaD3M frames AutoML pipeline synthesis as a single-player sequential decision-making problem inspired by AlphaZero. Instead of relying on explicit meta-features or algorithm rankings, it learns an implicit meta-representation of datasets and tasks through self-play, using neural networks and Monte-Carlo Tree Search to iteratively edit pipelines. This approach achieves competitive performance while being significantly faster and explainable by design.*

---

# *Paper4 : Automatic Exploration of Machine Learning Experiments on OpenML

**Link:** [Automatic Exploration of Machine Learning Experiments on OpenML](state_of_the_art/Automatic_Exploration_of_Machine_Learning_Experiments_on_OpenML.pdf)

**Daniel K√ºhn, Philipp Probst, Janek Thomas, Bernd Bischl**

---

## üìå Contexto en el estado del arte

Este paper se sit√∫a en la l√≠nea de:

* **Meta-learning basado en experiencias**
* **An√°lisis emp√≠rico de hiperpar√°metros**
* **Benchmarking masivo y reproducible**
* **OpenML como infraestructura cient√≠fica**

Es un paper **fundacional** para:

* meta-learning moderno,
* AutoML,
* y an√°lisis de *hyperparameter importance*.

üëâ A diferencia de AlphaD3M, **NO propone un nuevo algoritmo**, sino que crea **infraestructura experimental a gran escala**.

---

## üéØ ¬øDe qu√© trata el paper?

El paper introduce un **meta-dataset masivo** construido autom√°ticamente que:

* ejecuta **millones de experimentos ML**
* sobre **datasets reales de OpenML**
* con **muestreo aleatorio de hiperpar√°metros**
* de forma **totalmente autom√°tica** mediante el **OpenML Random Bot**

El objetivo central es:

> Entender emp√≠ricamente c√≥mo los hiperpar√°metros influyen en el rendimiento de los algoritmos.

---

## üß© Idea central

### üîë Contribuci√≥n clave

> **La comunidad necesita grandes bases de datos experimentales para estudiar ML emp√≠ricamente**, no solo benchmarks peque√±os.

Este trabajo:

* genera esa base de datos
* la publica
* la integra con OpenML

---

## üß™ ¬øQu√© datos usa?

### ‚úîÔ∏è Datasets

* **38 datasets de OpenML**
* Clasificaci√≥n supervisada
* Datasets p√∫blicos, variados y reales

‚ö†Ô∏è Limitaci√≥n importante:

* **38 datasets ‚â† diversidad extrema**
* pero **much√≠simos experimentos por dataset**

---

## ‚öôÔ∏è ¬øQu√© algoritmos eval√∫a?

Eval√∫a **6 algoritmos cl√°sicos**, elegidos por:

* estabilidad
* popularidad
* interpretabilidad

T√≠picamente (seg√∫n el paper y contexto OpenML):

* Random Forest
* Support Vector Machines
* k-Nearest Neighbors
* Decision Trees
* Naive Bayes
* Logistic Regression

üëâ **Aqu√≠ s√≠ hay algoritmos expl√≠citos**, aunque el foco no es compararlos, sino **estudiar su espacio de hiperpar√°metros**.

---

## üîß ¬øC√≥mo se generan los experimentos?

### üîÅ OpenML Random Bot

Un *bot autom√°tico* que:

1. Selecciona un dataset OpenML
2. Selecciona un algoritmo
3. Muestra aleatoriamente hiperpar√°metros
4. Ejecuta validaci√≥n cruzada
5. Sube los resultados a OpenML

### Escala:

* Hasta **20.000 configuraciones por algoritmo y dataset**
* ‚âà **2.5 millones de runs**
* Cada run:

  * algoritmo
  * hiperpar√°metros
  * score
  * tiempo
  * dataset_id
  * task_id

üëâ Todo queda **versionado y reproducible en OpenML**.

---

## üß† ¬øQu√© parte del meta-learning aborda?

Este paper es **meta-learning de nivel bajo (experimental)**.

### ‚úîÔ∏è Lo que S√ç cubre:

* Construcci√≥n de una **experience database**
* Relaci√≥n:

  ```
  (dataset, algoritmo, hiperpar√°metros) ‚Üí performance
  ```
* Base para:

  * algorithm selection
  * hyperparameter importance
  * surrogate models
  * AutoML

### ‚ùå Lo que NO cubre:

* No entrena meta-modelos
* No propone ranking de algoritmos
* No hace recomendaciones directamente

üëâ Es **infraestructura**, no el meta-learner.

---

## üß† Meta-features

‚ö†Ô∏è Punto importante (y frecuente confusi√≥n):

* **NO calcula meta-features del dataset**
* **NO las necesita**

Porque el objetivo es:

> estudiar el *response surface* de hiperpar√°metros, no predecir entre datasets.

---

## üìà Resultados y an√°lisis

El paper muestra:

* distribuciones de rendimiento
* sensibilidad a hiperpar√°metros
* regiones estables vs inestables
* interacci√≥n entre hiperpar√°metros

üëâ Conclusi√≥n clave:

> El rendimiento depende fuertemente de pocas configuraciones bien elegidas, justificando AutoML.

---

## üîó C√≥digo y datos

### ‚úîÔ∏è Datos

* Todos los resultados est√°n en **OpenML**
* Accesibles v√≠a:

  * OpenML API
  * `openml-python`
  * runs hist√≥ricos

### ‚úîÔ∏è C√≥digo

* Scripts del Random Bot (hist√≥ricos)
* Infraestructura OpenML

No hay un ‚Äúrepo bonito‚Äù, pero **los datos est√°n completamente disponibles**.

---

## üß† Relaci√≥n con otros trabajos

| Trabajo        | Rol                       |
| -------------- | ------------------------- |
| Brazdil et al. | Meta-learning cl√°sico     |
| AlphaD3M       | S√≠ntesis de pipelines     |
| AutoSklearn    | Optimizaci√≥n              |
| **Este paper** | Base experimental         |
| PIPES          | Meta-dataset de pipelines |

üëâ Este paper es **la capa base** sobre la que se apoyan muchos otros.

---

## üìù Resumen corto (para *Related Work*)

> *K√ºhn et al. present a large-scale experimental meta-dataset generated via automated random sampling of hyperparameters on OpenML. Covering millions of runs across multiple datasets and algorithms, this work provides the empirical foundation required for studying hyperparameter effects, algorithm behavior, and for training meta-learning and AutoML systems, rather than proposing a new meta-learning method itself.*

---

## üß† Diferencia clave con AlphaD3M

| AlphaD3M            | Este paper      |
| ------------------- | --------------- |
| M√©todo nuevo        | Infraestructura |
| RL + MCTS           | Random sampling |
| Pipelines completos | Algoritmos + HP |
| Impl√≠cito           | Expl√≠cito       |
| End-to-end          | Base de datos   |

---

## üéØ Para TU proyecto

Este paper es **perfecto** para justificar:

* uso de OpenML
* necesidad de meta-datasets grandes
* an√°lisis emp√≠rico
* reproducibilidad

Y combina muy bien con:

* Brazdil (ranking)
* PIPES (pipelines)
* AlphaD3M (s√≠ntesis)

---

# **Paper5 : üß† On the Predictive Power of Meta-Features in OpenML

**Link:** [ On the Predictive Power of Meta-Features in OpenML](state_of_the_art/Bilalli%20et%20al.pdf)

**Besim Bilalli, Alberto Abell√≥, Tom√†s Aluja-Banet (UPC, BarcelonaTech)**

---

## üìå Contexto en el estado del arte

* Este paper se sit√∫a en la l√≠nea de **meta-learning basado en meta-features**.
* Problema central:

  > Selecci√≥n autom√°tica de algoritmos (model/algorithm selection) depende de la **caracterizaci√≥n del dataset** mediante meta-features.
* Destinado a **asistir usuarios no expertos** en la selecci√≥n de modelos.

A diferencia de AlphaD3M, aqu√≠ **no se construyen pipelines ni se usa RL**; el enfoque es **anal√≠tico y estad√≠stico**, centrado en **meta-features predictivas**.

---

## üéØ ¬øDe qu√© trata el paper?

* Analiza la **capacidad predictiva de diferentes meta-features** en OpenML.
* Usa **factor analysis** para:

  1. Extraer **latent features** (agrupaciones de meta-features con caracter√≠sticas comunes)
  2. Evaluar su relaci√≥n con el rendimiento de 4 algoritmos de clasificaci√≥n en cientos de datasets
  3. Seleccionar las **latent features m√°s predictivas**
* Finalmente, realiza **meta-learning** usando las latent features seleccionadas para mejorar la recomendaci√≥n de algoritmos.

---

## üß© Idea central

> **Mejorar la efectividad del meta-learning** mediante la identificaci√≥n de las meta-features m√°s predictivas, usando an√°lisis estad√≠stico en datasets de OpenML.

* La aproximaci√≥n combina:

  * **Feature extraction**: factor analysis ‚Üí latent features
  * **Feature selection**: elegir las m√°s predictivas
  * **Meta-learning**: usar esas features para predecir el rendimiento de algoritmos

---

## üß™ Datos y experimentos

* **Datasets**: cientos de datasets p√∫blicos en OpenML
* **Algoritmos evaluados**: 4 algoritmos de clasificaci√≥n (no se listan todos expl√≠citamente)
* **Evaluaci√≥n**: relaci√≥n entre latent features y 3 m√©tricas de desempe√±o (accuracy, f1, etc.)
* **Resultado**: selecci√≥n de latent features con alto poder predictivo

---

## üîß Qu√© parte del meta-learning aborda

### ‚úîÔ∏è Lo que s√≠ hace:

* Meta-learning basado en **caracterizaci√≥n del dataset**
* Extracci√≥n de **features latentes predictivas**
* Mejora de la **predicci√≥n del algoritmo √≥ptimo**

### ‚ùå Lo que NO hace:

* No construye pipelines completos
* No usa RL ni search-based AutoML
* No genera ranking directo en OpenML de forma exhaustiva

---

## üß† Contribuciones principales

### ‚úîÔ∏è Conceptuales

* Demuestra que **la elecci√≥n de meta-features es cr√≠tica** para meta-learning
* Introduce **latent features** como representaci√≥n compacta y predictiva

### ‚úîÔ∏è T√©cnicas

* Uso de **factor analysis** para agrupar meta-features
* Selecci√≥n basada en relaci√≥n estad√≠stica con rendimiento de algoritmos

### ‚úîÔ∏è Pr√°cticos

* Dise√±a una **aplicaci√≥n para recuperar meta-datos de OpenML**
* Mejora procesos de **algorithm recommendation** para usuarios no expertos

---

## üìà Resultados clave

* Algunas latent features explican gran parte de la variabilidad en performance
* Meta-learning con features seleccionadas **mejora la recomendaci√≥n de algoritmos**
* Validaci√≥n emp√≠rica en **hundreds of OpenML datasets**

---

## üîó Implementaci√≥n / Datos

* **Datos y meta-features** disponibles en OpenML
* Aplicaci√≥n para **extraer meta-data**: facilita replicaci√≥n de experiments
* C√≥digo espec√≠fico **no publicado** como repo, pero los datos son accesibles

---

## üß† Comparaci√≥n con otros trabajos

| Trabajo            | Rol                                  |
| ------------------ | ------------------------------------ |
| Brazdil et al.     | Meta-learning cl√°sico (kNN, ranking) |
| AlphaD3M           | Pipeline synthesis y RL              |
| K√ºhn et al.        | Random hyperparameter experiments    |
| **Bilalli et al.** | Meta-feature selection y prediction  |

üëâ Este paper es **una pieza clave para elegir qu√© features usar** antes de entrenar un meta-learner.

---

## üìù Resumen corto (para *Related Work*)

> *Bilalli et al. study the predictive power of meta-features in OpenML. By extracting latent features through factor analysis and selecting the most predictive ones, they demonstrate improved meta-learning performance in algorithm recommendation. Unlike pipeline-synthesis approaches, this work focuses on understanding dataset characterizations and how they relate to algorithm performance.*

---

# *Paper6 : üß† Characterizing the Applicability of Classification Algorithms Using Meta-Level Learning

**Link:** [Characterizing the Applicability of Classification Algorithms Using Meta-Level Learning](state_of_the_art/characterizing-the-applicability-of-classification-4gmkiy2ggj.pdf)

**Pavel Brazdil, Jo√£o Gama, Bob Henery**

---

## üìå Contexto en el estado del arte

* Este paper es un **cl√°sico del meta-learning** aplicado a la **selecci√≥n de algoritmos de clasificaci√≥n**.

* Problema central:

  > Dado un nuevo dataset, ¬øqu√© algoritmos de clasificaci√≥n son m√°s adecuados?

* La idea es usar **informaci√≥n previa (meta-level)** sobre datasets y algoritmos para **generar recomendaciones autom√°ticas**.

---

## üéØ ¬øDe qu√© trata el paper?

* Realiza un **estudio comparativo** de distintos algoritmos: machine learning, estad√≠sticos y redes neuronales.

* Utiliza **meta-level learning**, es decir:

  1. Caracteriza datasets mediante **medidas estad√≠sticas e informaci√≥n te√≥rica**.
  2. Combina estas caracter√≠sticas con los resultados de tests previos.
  3. Entrena un **sistema de meta-learning** para predecir qu√© algoritmos son m√°s adecuados para un dataset dado.

* El sistema genera **reglas autom√°ticas**, que incluso pueden ser editadas por un usuario.

---

## üß© Idea central

> Usar resultados emp√≠ricos de algoritmos previos + caracter√≠sticas de datasets para entrenar un meta-modelo que recomiende algoritmos de clasificaci√≥n adecuados para nuevos datasets.

* **Meta-features utilizadas**: estad√≠sticas, medidas de informaci√≥n, propiedades de la distribuci√≥n de datos.
* **Meta-modelo**: machine learning aplicado sobre meta-datos, generando reglas y puntuaciones informativas.

---

## üß™ Datos y experimentos

* Datasets diversos (no especifica todos, t√≠pico de papers cl√°sicos de Brazdil).
* Algoritmos evaluados: varios clasificadores cl√°sicos, incluyendo ML, estad√≠sticos y redes neuronales.
* Evaluaci√≥n:

  * Los datasets se caracterizan mediante medidas estad√≠sticas y de informaci√≥n.
  * El sistema aprende la relaci√≥n entre estas caracter√≠sticas y el desempe√±o de los algoritmos.
  * Se generan recomendaciones para datasets nuevos con un **information score**.

---

## üîß Qu√© parte del meta-learning aborda

### ‚úîÔ∏è Lo que s√≠ hace:

* **Algorithm selection** basado en meta-learning.
* **Construcci√≥n de meta-features** (dataset characterization).
* **Generaci√≥n de reglas** que explican la recomendaci√≥n de algoritmos.

### ‚ùå Lo que NO hace:

* No realiza s√≠ntesis de pipelines completos.
* No usa RL ni AutoML moderno.
* No trabaja con millones de runs como OpenML Random Bot.

---

## üß† Contribuciones principales

### ‚úîÔ∏è Conceptuales

* Introduce el concepto de **meta-level learning** aplicado a la selecci√≥n de algoritmos.
* Muestra c√≥mo combinar **caracter√≠sticas del dataset + desempe√±o previo** para predecir la idoneidad de algoritmos.

### ‚úîÔ∏è T√©cnicas

* Medidas estad√≠sticas e informaci√≥n te√≥rica como meta-features.
* Sistema de **reglas autom√°ticas** generadas por ML.

### ‚úîÔ∏è Pr√°cticos

* Herramienta para recomendar algoritmos en datasets nuevos.
* Mejora la experiencia de usuarios no expertos.

---

## üìà Resultados clave

* El sistema puede **predecir qu√© algoritmos son m√°s adecuados** para un dataset nuevo.
* Los scores de informaci√≥n permiten **clasificar y priorizar algoritmos**.
* Experimentos muestran que **las recomendaciones son √∫tiles y viables** en la pr√°ctica.

---

## üîó Implementaci√≥n / Datos

* No hay repo publicado moderno.
* Experimentos y meta-features disponibles en el paper y referencias hist√≥ricas de Brazdil.
* Idea replicable usando datasets de OpenML y caracter√≠sticas estad√≠sticas.

---

## üß† Relaci√≥n con otros trabajos

| Trabajo                 | Rol                                              |
| ----------------------- | ------------------------------------------------ |
| **Brazdil et al. 2003** | Meta-learning cl√°sico de selecci√≥n de algoritmos |
| Bilalli et al.          | Meta-feature extraction y predicci√≥n             |
| AlphaD3M                | S√≠ntesis de pipelines y AutoML                   |
| K√ºhn et al.             | Dataset-algorithm meta-dataset                   |

> Este paper es un **punto de partida hist√≥rico** para meta-learning basado en dataset characterization y algorithm recommendation.

---

## üìù Resumen corto (para *Related Work*)

> *Brazdil et al. present a meta-learning approach to algorithm selection. Using dataset characterization through statistical and information-theoretic measures, combined with previous algorithm performance, the system generates rules and information scores to recommend suitable classifiers for new datasets. This work lays the foundation for meta-feature based algorithm recommendation in modern AutoML pipelines.*

---

# **Paper 7: üß† Experiment Databases (Vanschoren et al., 2009/2011)

**Link:** [üß† Experiment Databases (Vanschoren et al., 2009/2011)](state_of_the_art/Experiment_databases.pdf)


## üìå Contexto

* Muchos papers de ML generan resultados experimentales, pero **gran parte de los detalles se pierden** tras la publicaci√≥n.

* Esto dificulta:

  * Reproducibilidad
  * Reutilizaci√≥n de experimentos para nuevos estudios
  * Comparaciones sistem√°ticas entre algoritmos y datasets

* Soluci√≥n propuesta: **bases de datos de experimentos** (Experiment Databases) que almacenan:

  * Datasets
  * Algoritmos
  * Hiperpar√°metros
  * Resultados de evaluaci√≥n

---

## üéØ ¬øDe qu√© trata el paper?

* Presenta un **framework colaborativo** para almacenar y compartir resultados experimentales de ML.

* Organiza experimentos autom√°ticamente en **bases de datos p√∫blicas**, permitiendo:

  * Reutilizar experimentos previos
  * Analizar resultados a gran escala
  * Responder preguntas de investigaci√≥n sobre algoritmos, hiperpar√°metros y datasets

* Actualmente contiene **m√°s de 650,000 experimentos de clasificaci√≥n**.

---

## üß© Relevancia para tu proyecto

Si tu objetivo es **meta-learning con metafeatures** para recomendar algoritmos, hiperpar√°metros o pipelines:

1. **Base de datos de experimentos = fuente de meta-datos**

   * Cada registro contiene:

     * Dataset (con sus caracter√≠sticas/metafeatures)
     * Algoritmo usado
     * Hiperpar√°metros
     * Resultado obtenido (accuracy, F1, etc.)

2. **Permite entrenar un meta-learner**

   * Puedes usar los metadatos para predecir:

     * Qu√© algoritmo funcionar√° mejor en un dataset nuevo
     * Qu√© combinaci√≥n de hiperpar√°metros es prometedora
     * Incluso qu√© pipeline ser√≠a adecuado si combinas algoritmos y preprocesamiento

3. **Facilita reproducibilidad y comparaciones**

   * Puedes validar tus recomendaciones comparando con resultados ya almacenados

---

## üîß Qu√© NO hace este paper

* No propone **algoritmos de AutoML**
* No construye pipelines autom√°ticamente
* No aplica directamente meta-learning, aunque **los datos que organiza son perfectos para hacerlo**

---

## ‚úÖ Conclusi√≥n para tu proyecto

Tiene el estilo de **PIPES** y de **OpenML**, solo que con matices:

* **OpenML**:

  * Plataforma actual, online y colaborativa.
  * Permite **almacenar datasets, runs de algoritmos, metafeatures, experimentos completos**.
  * Facilita **descargar datasets, resultados y metafeatures para meta-learning o AutoML**.

* **PIPES**

  * Es un **framework de experimentaci√≥n y evaluaci√≥n de pipelines**, m√°s enfocado en **evaluar algoritmos y generar recomendaciones** basadas en metafeatures.
  * Incluye **algoritmos evaluados, rankings y pipelines sugeridos**, m√°s cercano a un **sistema de recomendaci√≥n**.

* **Experiment Databases (Vanschoren et al., 2009)**:

  * Es **el antecesor conceptual de OpenML**.
  * Base de datos de experimentos para **guardar, organizar y compartir resultados de ML**.
  * No ejecuta pipelines ni hace recomendaciones autom√°ticas, pero **los datos que contiene permiten entrenar meta-learners o sistemas de recomendaci√≥n**.

---

üí° **Resumen comparativo simple**:

| Sistema / Base       | Qu√© hace                                                          | Meta-learning √∫til para tu proyecto?                            |
| -------------------- | ----------------------------------------------------------------- | --------------------------------------------------------------- |
| OpenML               | Almacena datasets, runs, metafeatures; API para acceso y descarga | ‚úÖ Directamente, listo para entrenar meta-learners               |
| PIPES                | Eval√∫a pipelines y algoritmos; genera rankings                    | ‚úÖ S√≠, m√°s cercano a recomendaci√≥n autom√°tica                    |
| Experiment Databases | Solo guarda y organiza resultados experimentales                  | ‚úÖ S√≠, pero necesitas construir tu meta-learner sobre esos datos |

---

# **Paper8: üß† Fast Algorithm Selection using Learning Curves

**Link:** [üß† Fast Algorithm Selection using Learning Curves](state_of_the_art/Fast_Learning_curve.pdf)

**Jan N. van Rijn, Salisu Mamman Abdulrahman, Pavel Brazdil, Joaquin Vanschoren**

---

## üìå Contexto en el estado del arte

* Problema central:

  > Encontrar un clasificador y su configuraci√≥n de hiperpar√°metros que funcionen bien en un dataset dado.

* Dificultad: Evaluar todas las combinaciones posibles **toma demasiado tiempo**.

* Soluci√≥n: Predecir qu√© algoritmos son m√°s prometedores a partir de **peque√±as muestras de datos**.

---

## üéØ Idea central del paper

* El objetivo es **rankear algoritmos en lugar de clasificarlos**:

  * La primera recomendaci√≥n no siempre es la mejor
  * Se generan **m√∫ltiples recomendaciones** basadas en desempe√±o medido sobre peque√±as muestras

* Introduce el concepto de **Loss-Time Curves**:

  * Visualizan **cu√°nto tiempo (budget)** se necesita para llegar a una soluci√≥n aceptable
  * Permite evaluar rankings de algoritmos considerando **tiempo y rendimiento**

* El m√©todo propuesto:

  1. Toma peque√±as muestras de un dataset.
  2. Eval√∫a r√°pidamente los clasificadores.
  3. Genera un ranking adaptado para minimizar tiempo y maximizar precisi√≥n.

---

## üß© Datos y experimentos

* Se usan **datasets de benchmark** (probablemente OpenML, aunque no especifica todos).
* Clasificadores evaluados: varios **algoritmos cl√°sicos de ML**.
* Resultados:

  * El m√©todo converge **muy r√°pido** a soluciones aceptables
  * Permite comparar rankings de algoritmos considerando **tiempo de entrenamiento y precisi√≥n**

---

## üîß Qu√© parte del meta-learning aborda

### ‚úîÔ∏è Lo que hace:

* **Algorithm selection** basado en rendimiento medido sobre peque√±as muestras.
* **Ranking de algoritmos**, no solo predicci√≥n del mejor.
* Considera **trade-off entre tiempo y precisi√≥n**.

### ‚ùå Lo que NO hace:

* No hace s√≠ntesis completa de pipelines (solo selecciona algoritmos).
* No extrae nuevas metafeatures, sino que usa caracter√≠sticas existentes de datasets.

---

## ‚úÖ Contribuciones principales

### ‚úîÔ∏è Conceptuales

* Introduce **evaluaci√≥n de rankings de algoritmos con Loss-Time Curves**.
* Propone un **meta-approach r√°pido** para selecci√≥n de algoritmos basado en subsampling.

### ‚úîÔ∏è T√©cnicas

* Mide rendimiento de clasificadores en **subsets del dataset**.
* Genera **ranking adaptado a tiempo y precisi√≥n**.

### ‚úîÔ∏è Pr√°cticos

* Permite **seleccionar algoritmos prometedores r√°pidamente**.
* √ötil para **AutoML y meta-learning** cuando evaluar todos los algoritmos es costoso.

---

## üîó Relevancia para el proyecto

* Muy relevante para **meta-learning basado en metafeatures y selecci√≥n de algoritmos**.
* Conceptos aplicables:

  * Usar **peque√±as muestras** para predecir rendimiento de algoritmos
  * Generar **ranking de algoritmos** en lugar de solo seleccionar uno
  * Considerar **tiempo como parte del criterio** de selecci√≥n

---

# *Paper9:üß† PIPES: A Meta-dataset of Machine Learning Pipelines

**Link:** [üß† PIPES: A Meta-dataset of Machine Learning Pipelines](state_of_the_art/PIPES:a_meta-dataset_of_ML.pdf)

**Cynthia Moreira Maia, Lucas B. V. de Amorim, George D. C. Cavalcanti, Rafael M. O. Cruz**

---

## üìå Contexto

* Problema central:

  > Los sistemas de meta-learning requieren datasets de referencia con informaci√≥n sobre algoritmos, hiperpar√°metros y pipelines completos para entrenar meta-learners.

* Dificultad:

  * Los datasets de meta-learning existentes **no contienen pipelines completos ni metafeatures detalladas**.
  * La evaluaci√≥n de pipelines sobre muchos datasets es **costosa y poco reproducible**.

* Soluci√≥n: **PIPES**, un meta-dataset que organiza informaci√≥n sobre pipelines de ML y sus resultados, optimizado para meta-learning y recomendaci√≥n autom√°tica.

---

## üéØ Idea central

* PIPES es un **meta-dataset de pipelines de ML** que contiene:

  * Datasets tabulares de benchmark
  * Metafeatures de los datasets
  * Algoritmos usados en pipelines (clasificaci√≥n y regresi√≥n)
  * Hiperpar√°metros
  * Rendimiento de cada pipeline (accuracy, RMSE, etc.)

* Incluye **informaci√≥n estructurada** para facilitar:

  * Entrenamiento de meta-learners
  * Predicci√≥n del rendimiento de pipelines
  * Recomendaci√≥n de algoritmos y configuraciones

* Tambi√©n proporciona una **API** para acceder a los datos y realizar consultas sobre datasets, algoritmos y metafeatures.

---

## üß© Datos y experimentos

* Contiene **pipelines completos**, desde preprocesamiento hasta estimadores.
* Pipelines evaluados sobre **varios datasets tabulares** (principalmente de OpenML).
* Permite an√°lisis de **eficiencia, reproducibilidad y generalizaci√≥n de pipelines**.

---

## üîß Qu√© parte del meta-learning aborda

### ‚úîÔ∏è Lo que hace:

* **Recomendaci√≥n de pipelines y algoritmos** basada en metafeatures.
* Facilita **aprendizaje meta sobre rendimiento hist√≥rico** de algoritmos y configuraciones.
* Permite construir **modelos predictivos sobre qu√© pipeline funcionar√° mejor** en un dataset nuevo.

### ‚ùå Lo que NO hace:

* No genera pipelines autom√°ticamente; **proporciona datos para entrenar un meta-learner que lo haga**.
* No propone nuevas arquitecturas de aprendizaje; **es una base de datos/meta-dataset**.

---

## ‚úÖ Contribuciones principales

1. **Meta-dataset completo de pipelines** para clasificaci√≥n y regresi√≥n.
2. Incluye **metafeatures, algoritmos, hiperpar√°metros y resultados** para cada dataset.
3. Proporciona **API y estructura reproducible** para experimentos de meta-learning.
4. Facilita **evaluaci√≥n de recomendaciones y an√°lisis de algoritmos/pipelines**.

---

## üîó Relevancia para tu proyecto

* Muy √∫til si quieres entrenar un **meta-learner que prediga el mejor pipeline o algoritmo** para un dataset nuevo usando metafeatures.
* Combina lo mejor de **OpenML (datasets y metafeatures)** con la **evaluaci√≥n hist√≥rica de pipelines**, similar a lo que propon√≠a **Experiment Databases** pero m√°s estructurado para pipelines completos.
* Permite **reproducibilidad y benchmarking** de algoritmos y configuraciones.

---

# **Paper10: üß† Pairwise Meta‚ÄëRules for Better Meta‚ÄëLearning‚ÄëBased Algorithm Ranking

**Link:** [üß† Pairwise Meta‚ÄëRules for Better Meta‚ÄëLearning‚ÄëBased Algorithm Ranking](state_of_the_art/Quan_Sun.pdf)

**Quan Sun ¬∑ Bernhard Pfahringer**

---

## üìå Contexto general

* El problema central del paper es **algorithm selection / ranking**:

  > Dado un nuevo dataset, ¬øqu√© algoritmos probablemente funcionar√°n mejor?

* En meta‚Äëlearning, esto suele abordarse aprendiendo de **experiencias previas** (datasets + rendimiento de algoritmos) para predecir rankings en datasets nuevos.

* El **objetivo espec√≠fico** de este trabajo es mejorar c√≥mo se construyen esos rankings usando **reglas meta** m√°s robustas, basadas en comparaciones **par a par** entre algoritmos.

---

## üéØ Idea central del paper

### ü§î Problema al que responde

* Muchos enfoques de meta‚Äëlearning intentan predecir **el mejor algoritmo** directamente o producen un **ranking global** de algoritmos.
* Sin embargo, esos enfoques:

  * pueden ser sensibles a particularidades de la m√©trica
  * pueden ignorar relaciones complementarias entre algoritmos
  * pueden fallar al generalizar a datasets nuevos

### üîç La soluci√≥n propuesta

El paper propone construir el ranking de algoritmos usando **reglas basadas en comparaciones par a par** de desempe√±o:

> En lugar de intentar predecir ‚Äúel mejor algoritmo‚Äù, la idea es aprender **reglas meta** que predicen si A > B, B > C, etc., para cada par de algoritmos.

Esto genera un **ranking m√°s estable** y con mejor calidad de ordenaci√≥n cuando hay muchos algoritmos posibles.

---

## üß† ¬øQu√© significa *pairwise meta‚Äërules*?

* Una **meta‚Äëregla par a par** es una regla que compara 2 algoritmos con base en meta‚Äëfeatures del dataset.

* Por ejemplo:

  * ‚ÄúSi el n√∫mero de instancias es mayor a X y la entrop√≠a de la clase es menor a Y, entonces A es mejor que B‚Äù
  * Estas reglas se aprenden usando meta‚Äëdatos hist√≥ricos.

* **Meta‚Äëfeatures del dataset** pueden ser:

  * n√∫mero de atributos
  * n√∫mero de instancias
  * proporci√≥n de clases
  * medidas estad√≠sticas
  * etc.

---

## üß™ ¬øC√≥mo funciona el m√©todo?

1. **Recolectar meta‚Äëdatos hist√≥ricos**

   * Para muchos datasets:
     `{dataset_meta_features, rendimiento_algoritmos}`

2. **Construir comparaciones parejas**

   * Para cada par `(Alg_i, Alg_j)`, crear ejemplos de entrenamiento:

     * Si Alg_i fue mejor ‚Üí etiqueta ‚Äúi > j‚Äù
     * Si Alg_j fue mejor ‚Üí etiqueta ‚Äúj > i‚Äù

3. **Entrenar meta‚Äëclasificadores par a par**

   * Con meta‚Äëfeatures del dataset como entradas
   * Con la comparaci√≥n (i mejor que j) como salida

4. **Construir ranking para un dataset nuevo**

   * Aplicar los meta‚Äëclasificadores par a par
   * Combinar resultados para obtener el ranking final

---

## üîß ¬øQu√© datos / algoritmos se usan?

* El paper est√° t√≠picamente evaluado con:

  * Datasets p√∫blicos (open benchmarks)
  * Algoritmos cl√°sicos de clasificaci√≥n
  * Las m√©tricas de desempe√±o pueden ser accuracy u otras m√©tricas relevantes

‚ö†Ô∏è El foco **no es un conjunto espec√≠fico de datasets o pipelines**, sino **las reglas generadas**.

---

## üß† ¬øQu√© parte del Meta‚ÄëLearning aborda?

### ‚úîÔ∏è Lo que hace

* **Algorithm Ranking** usando meta‚Äëlearning
* **Construcci√≥n de reglas interpretables** para comparar pares de algoritmos
* Uso de **meta‚Äëfeatures del dataset** para alimentar esas reglas
* Evaluaci√≥n de ranking en datasets de benchmark

### ‚ùå Lo que NO hace

* No genera pipelines completos
* No usa t√©cnicas de AutoML avanzado
* No se focaliza en hiperpar√°metros
* Se centra en ranking de algoritmos individuales

---

## ‚ñ∂Ô∏è ¬øQu√© aporta respecto a m√©todos cl√°sicos?

Las contribuciones clave son:

### üìå 1. Mejor calidad de ranking

* Los rankings generados a partir de reglas par a par suelen ser m√°s robustos y generalizables.
* Est√°n menos afectados por ruido o problemas con m√©tricas.

### üìå 2. Interpretabilidad

* Las reglas par a par pueden **interpretarse** f√°cilmente:

  * ‚ÄúSi la dimensionalidad es X y la clase est√° desequilibrada, entonces algoritmo A tiende a superar a B‚Äù.

### üìå 3. Escalabilidad

* Este enfoque puede escalar mejor a muchos algoritmos que tratar de predecir un ranking completo de una vez.

---

## üìà Ejemplo simplificado

Si tenemos 3 algoritmos: A, B, C

| Dataset | A vs B | A vs C | B vs C |
| ------- | ------ | ------ | ------ |
| d1      | A > B  | A > C  | C > B  |
| d2      | B > A  | C > A  | C > B  |
| d3      | A > B  | C > A  | C > B  |

Entonces:

* Para cada par (A, B), (A, C), (B, C), entrenas un meta‚Äëclasificador.
* Para un dataset nuevo:

  * Estimas si A > B, A > C, B > C
  * Combinas esas predicciones ‚Üí ranking final

---

## üß† ¬øPor qu√© es relevante para tu proyecto?

Este paper es muy √∫til si tu objetivo es:

* entrenar un sistema que **no solo elija un algoritmo**, sino que **genere un ranking** ordenado de algoritmos
* **comprender qu√© caracter√≠sticas de un dataset favorecen un algoritmo frente a otro**
* desarrollar meta‚Äëlearners **explicables**
* combinar resultados de muchos pares para producir una recomendaci√≥n robusta

---

## üìù Resumen para tu estado del arte

> *Sun & Pfahringer proponen un m√©todo de meta‚Äëlearning para ranking de algoritmos que se basa en reglas par a par entre algoritmos, construidas usando meta‚Äëfeatures de datasets. Este enfoque mejora la robustez e interpretabilidad del ranking frente a m√©todos convencionales de selecci√≥n de algoritmos.*

---

## üìå En una frase para tu proyecto

> *‚ÄúEste trabajo usa comparaciones par a par entre algoritmos, basadas en meta‚Äëfeatures, para construir rankings m√°s robustos y explicables para meta‚Äëlearning en selecci√≥n de algoritmos.‚Äù*

---

# *Paper11: üß† Ranking Learning Algorithms: Using IBL and Meta-Learning on Accuracy and Time Results

**Link:** [üß† anking Learning Algorithms: Using IBL and Meta-Learning on Accuracy and Time Results](state_of_the_art/Ranking_Learning_Algorithms_Using_IBL_and_Meta-Lea.pdf)


**Autores:** Pavel B. Brazdil, Carlos Soares, Joaquim Pinto da Costa
**Afiliaci√≥n:** LIACC, University of Porto, Portugal
**Keywords:** algorithm recommendation, meta-learning, data characterization, ranking

---

## üìå Contexto general

* El paper aborda **la selecci√≥n de algoritmos de Machine Learning** mediante **meta-learning**, considerando **precisi√≥n y tiempo de ejecuci√≥n**.
* El problema: elegir el algoritmo m√°s adecuado para un dataset nuevo bas√°ndose en experiencias previas de otros datasets similares.

---

## üéØ Idea central del paper

### üîπ Problema al que responde

* No todos los datasets son iguales; un algoritmo puede funcionar bien en uno y mal en otro.
* Seleccionar manualmente un algoritmo puede ser lento o poco efectivo.
* El objetivo es generar un **ranking de algoritmos candidato** basado en caracter√≠sticas del dataset.

### üîπ Soluci√≥n propuesta

1. **Representaci√≥n de datasets mediante meta-features**

   * Se elige un **peque√±o conjunto de caracter√≠sticas de los datos** que influyen en el desempe√±o de los algoritmos, por ejemplo:

     * n√∫mero de instancias
     * n√∫mero de atributos
     * proporci√≥n de clases
     * otras medidas estad√≠sticas o de complejidad del dataset

2. **Identificaci√≥n de datasets similares**

   * Se usa **k-Nearest Neighbor (k-NN)** para encontrar datasets previos **similares al dataset actual** seg√∫n las meta-features.

3. **Generaci√≥n de ranking de algoritmos**

   * Se toman los **rendimientos de los algoritmos en los datasets similares**.
   * Se crea un **ranking multicriterio** que considera:

     * **Accuracy** (precisi√≥n)
     * **Tiempo de ejecuci√≥n**

4. **Evaluaci√≥n de rankings**

   * Se adapta una **metodolog√≠a estad√≠stica para evaluar rankings**, ya que no es com√∫n trabajar con rankings directamente en ML.
   * Comparan su m√©todo con un **ranking base** y muestran mejoras significativas.

---

## üß† Qu√© aporta este paper

### ‚úîÔ∏è Principales aportes

1. **Uso de meta-learning para ranking multicriterio**

   * Integra **precisi√≥n y tiempo de ejecuci√≥n** en un √∫nico ranking.
2. **Evaluaci√≥n de rankings**

   * Proponen una metodolog√≠a estad√≠stica general para evaluar rankings, aplicable a otros problemas de ranking.
3. **Recomendaci√≥n interpretativa**

   * El m√©todo proporciona al usuario un **ranking de algoritmos candidato** en lugar de un √∫nico algoritmo.

### ‚úîÔ∏è Relevancia para meta-learning

* Muestra c√≥mo se puede usar **informaci√≥n hist√≥rica de datasets** y **meta-features** para predecir el rendimiento de algoritmos en datasets nuevos.
* Aunque aqu√≠ se concentra en clasificaci√≥n, el enfoque es **generalizable a combinaciones de m√©todos o estrategias m√°s complejas**.

---

## üîπ Ejemplo conceptual

Supongamos que tienes 3 algoritmos: A, B y C y un dataset nuevo D:

1. Encuentras los datasets similares a D seg√∫n meta-features.
2. Observas c√≥mo A, B y C funcionaron en esos datasets.
3. Generas un ranking basado en desempe√±o y tiempo:

   * Dataset similar 1: B > A > C
   * Dataset similar 2: A > C > B
4. Combinando resultados ‚Üí ranking final recomendado para D.

---

## üß† Por qu√© es √∫til para tu proyecto

* **Selecci√≥n de algoritmos basada en meta-features** ‚Üí directamente aplicable a sistemas de recomendaci√≥n de pipelines.
* **Ranking multicriterio** ‚Üí √∫til si quieres considerar **m√°s de una m√©trica** (por ejemplo, precisi√≥n vs tiempo de entrenamiento).
* **Framework general de meta-learning** ‚Üí se puede extender a:

  * selecci√≥n de pipelines completos
  * selecci√≥n de hiperpar√°metros
  * integraci√≥n en AutoML

---

En resumen:

> Este trabajo propone un enfoque de meta-learning basado en k-NN para recomendar un **ranking de algoritmos de clasificaci√≥n**, usando meta-features de datasets y evaluando precisi√≥n y tiempo. La metodolog√≠a permite generar rankings m√°s precisos que m√©todos base y es aplicable a problemas de recomendaci√≥n m√°s amplios.

---

# Paper12 : üß† Tunability: Importance of Hyperparameters of Machine Learning Algorithms

**Link:** [üß† Tunability: Importance of Hyperparameters of Machine Learning Algorithms](state_of_the_art/Tunability_%20Importance%20of%20Hyperparameters%20of%20Machine.pdf)

**Autores:** Philipp Probst, Anne-Laure Boulesteix, Bernd Bischl
**Keywords:** hyperparameter tuning, tunability, meta-learning, OpenML, benchmarking

---

## üìå Contexto general

* Muchos algoritmos de **Machine Learning supervisado** dependen de **hiperpar√°metros** que deben configurarse antes de entrenar.

* Elegir valores adecuados puede mejorar significativamente el rendimiento.

* Opciones comunes:

  1. Valores por defecto del software
  2. Configuraci√≥n manual por el usuario
  3. Optimizaci√≥n autom√°tica mediante tuning

* El paper se centra en **cuantificar la importancia de los hiperpar√°metros** y evaluar qu√© tan ‚Äútunables‚Äù son.

---

## üéØ Idea central del paper

### üîπ Problema al que responde

* No todos los hiperpar√°metros son igual de importantes para todas las tareas.
* Necesidad de saber:

  * Qu√© hiperpar√°metros realmente afectan el rendimiento
  * Cu√°ndo vale la pena realizar tuning costoso
* Esto es crucial para **meta-learning y AutoML**, porque ayuda a priorizar recursos y decidir si ajustar par√°metros mejora el rendimiento.

### üîπ Soluci√≥n propuesta

1. **Formalizaci√≥n del concepto de tunabilidad**

   * Definen estad√≠sticamente cu√°nto impacta un hiperpar√°metro en el rendimiento esperado del algoritmo.
   * Introducen medidas generales para cuantificar la tunabilidad de cada par√°metro.

2. **Benchmarking a gran escala**

   * Usan **38 datasets de OpenML**.
   * Evaluaci√≥n de **6 algoritmos comunes**.
   * Para cada algoritmo, generan muchas configuraciones de hiperpar√°metros y comparan rendimientos.

3. **An√°lisis y recomendaciones**

   * Identifican **valores por defecto basados en datos**.
   * Determinan cu√°les par√°metros realmente influyen en el rendimiento.
   * Permiten decidir cu√°ndo **vale la pena realizar tuning** y cu√°les se pueden dejar en default.

---

## üß† Qu√© aporta este paper

### ‚úîÔ∏è Principales aportes

1. **Medidas cuantitativas de tunabilidad**

   * Permiten saber la importancia relativa de cada hiperpar√°metro.
   * Facilitan comparaciones entre algoritmos.

2. **Benchmarking extensivo**

   * Datos p√∫blicos de OpenML
   * Amplio an√°lisis de hiperpar√°metros en varios datasets

3. **Orientaci√≥n pr√°ctica**

   * Para AutoML y sistemas de meta-learning:

     * Identificar qu√© hiperpar√°metros priorizar
     * Reducir el tiempo computacional evitando tuning innecesario

---

## üîπ Ejemplo conceptual

* Algoritmo: Random Forest
* Hiperpar√°metros:

  * n_estimators, max_depth, min_samples_split
* Resultado:

  * n_estimators ‚Üí bajo impacto en performance (poco tunable)
  * max_depth ‚Üí alto impacto (muy tunable)
  * min_samples_split ‚Üí impacto moderado
* Con esto, un sistema de AutoML puede **centrarse en tunear max_depth y min_samples_split** y dejar n_estimators por defecto, ahorrando tiempo.

---

## üß† Relevancia para meta-learning y AutoML

* Permite **priorizar hiperpar√°metros cr√≠ticos** en un proceso de recomendaci√≥n de algoritmos o pipelines.
* Se puede integrar en sistemas como:

  * **AlphaD3M** ‚Üí priorizar hiperpar√°metros durante s√≠ntesis de pipelines
  * **PIPES** ‚Üí construir meta-datasets con foco en par√°metros m√°s importantes
* Mejora la **eficiencia de tuning autom√°tico** y ayuda a decidir si vale la pena un ajuste exhaustivo.

---

## üîπ Resumen para tu estado del arte

> *Probst et al. formalizan el concepto de ‚Äútunabilidad‚Äù de los hiperpar√°metros y proporcionan medidas para evaluar su impacto en el rendimiento de algoritmos. Usando datasets de OpenML, identifican qu√© par√°metros son cr√≠ticos y cu√°les pueden dejarse en valores por defecto, lo que es √∫til para meta-learning y AutoML.*

---

